---
title: "Project4_Group4"
author: "Nishanth Gandhidoss, Raghavendran Shankar"
date: "25 March 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r include=FALSE}
# installing the packages
# Function to Install packages
# checks the available.packages before
installNewPackage <- function(packageName) {
        if(packageName  %in% rownames(installed.packages()) == FALSE)
        {
                install.packages(packageName, repos = "http://cran.us.r-project.org", dependencies=TRUE)
        }
}

installNewPackage("caret")
installNewPackage("randomForest")
installNewPackage("rpart")
installNewPackage("MLmetrics")
installNewPackage("e1071")
installNewPackage("class")
installNewPackage("caTools")
installNewPackage("ROCR")
installNewPackage("pROC")
installNewPackage("plyr")
installNewPackage("adabag")
installNewPackage("readr")
installNewPackage("tm")
installNewPackage("dplyr")

library(caret)
library(randomForest)
library(rpart)
library(MLmetrics)
library(e1071)
library(class)
library(caTools)
library(ROCR)
library(pROC)
library(plyr)
library(adabag)
library(readr)
library(tm)
library(dplyr)
```


#### Holdover from last assignment

#### Question 8

#### Classification of Music Popularity

For this problem, you will work to classify a song's popularity. Specifically, you will develop methods to predict whether a song will make the Top10 of Billboard's Hot 100 Chart. The data set consists of song from the Top10 of Billboard's Hot 100 Chart from 1990-2010 along with a sampling of other songs that did not make the list1.

The variables included in the data set include several description of the song and artist (including song title and id numbers), the year the song was released. Additionally, several variables describe the song attributes: time signature, loudness, tempo, key, energy pitch, and timbre (measured of different sections of the song). The last variable is binary indicated whether the song was in the Top10 or not.

You will use the variables of the song attributes (excluding the variables involving confidence in that attribute, e.g., key confidence) to predict whether the song will be popular or not.


#### Question 8a

Load in the music data. You should not use the artist or song title and IDs in the prediction along with the confidence variables.


```{r question8a}
# Loading the music data
music_real <- read.csv("data/music.csv")

# Removing IDs, artist, song title variables out of the data 
music <-  music_real[, -c(2, 3, 4, 5, 6 , 9, 11)]

# Checking for NA values in the data
sapply(music, function(x) sum(is.na(x)))
```


#### Question 8b

Prepare the data for a 10-fold cross-validation. Ensure that each split of the data has a balanced distribution of class labels.


```{r question8b}
# Data preprocessing for 10 fold cross validation

# Setting the seed for reproduciablility
set.seed(294)

# NO of fold
k_fold <- 10

# Creating the folds variable in the data frame
# music$folds <- createFolds(music$Top10, k = n_fold, list = FALSE)
cv_index_list <- createFolds(music$Top10, k = k_fold, list = TRUE, returnTrain = FALSE)

# Checking the class label proportions
for(i in 1:k_fold) {
    fold <- music$Top10[cv_index_list[[i]]]
    fold_length <- length(fold)
    class0_prop <- round(length(fold[fold == 0]) / fold_length, 2)
    class1_prop <- round(length(fold[fold == 1]) /  fold_length, 2)
    print(paste("Fold" , i , "length", fold_length, sep = " "))
    print(paste("Class Label(0)", round(class0_prop, 2), sep = " "))
    print(paste("Class Label(1)", round(class1_prop, 2), sep = " "))
}
```


#### Question 8f

(15 points) Use Random Forests to predict whether a song is a hit. Calculate and report the accuracy, error, and AUC performance on the testing data.


```{r question8f}
# Intailize variables
total_acc <- c()
total_error_rate <- c()
total_AUC <- c()

# Running the Random Forest with 10 - fold cross validation
# Accuracy, error, AUC are rounded off to two decimal points
for(fold in 1:k_fold) {
    train_set <- music[-cv_index_list[[fold]], ]
    test_set <- music[cv_index_list[[fold]], ]

    # Fitting the model
    rf_fit <- randomForest(as.factor(Top10) ~ ., data = train_set, importance = TRUE, ntree = 100)

    # Calculating Accuracy. Error, AUC
    test_prediction <- predict(rf_fit, test_set, type = 'class')
    accuracy <- round(Accuracy(test_prediction, test_set$Top10), 2)
    error_rate <- 1 - accuracy
    AUC <- round(AUC(test_prediction, test_set$Top10), 2)
    total_acc <- c(total_acc, accuracy)
    total_error_rate <- c(total_error_rate, error_rate)
    total_AUC <- c(total_AUC, AUC)
}

report <- data.frame(Fold = c(1:10), Accuracy = total_acc, Error_rate = total_error_rate,
                     AUC = total_AUC)
print(report)

print(paste("Overall accuracy is", round(sum(total_acc) / k_fold, 2), sep = " "))
print(paste("Overall error is", round(sum(total_error_rate) / k_fold, 2), sep =" "))
print(paste("Overall area under the curve(AUC) is", round(sum(total_AUC) / k_fold, 2), sep = " "))
```


#### Question 8g

(20 points) Learn a support vector machine (SVM) with a RBF kernel to predict whether the song is a hit. Consider at least the following values for cost: 0.01, 0.1, 1, 10, 100. Calculate and report the accuracy, error, and AUC performance on the testing data for the best model found.


```{r question8g}
# Intializing the cost variables
cost_values <- c(0.01, 0.1, 1, 10, 100)
total_acc_list <- c()
total_error_rate_list <- c()
total_AUC_list <- c()

# Running the Support Vector machine for cost values
# 0.01, 0.1, 1, 10, 10, 100 with 10 - fold cross validation
# Accuracy, error, AUC are rounded off to two decimal points
for(cost in cost_values) {

    # Intailize variables
    total_acc <- c()
    total_error_rate <- c()
    total_AUC <- c()

    print(paste("*** For the cost value", cost, "***", sep = " "))

    for(fold in 1:k_fold) {
        train_set <- music[-cv_index_list[[fold]], ]
        test_set <- music[cv_index_list[[fold]], ]

        # Fitting the model
        svm_fit <- svm(as.factor(Top10) ~ ., data = train_set, kernel = "radial", cost = cost)

        # Calculating Accuracy. Error, AUC
        test_prediction <- predict(svm_fit, test_set, type = 'class')
        accuracy <- round(Accuracy(test_prediction, test_set$Top10), 2)
        error_rate <- 1 - accuracy
        AUC <- round(AUC(test_prediction, test_set$Top10), 2)
        total_acc <- c(total_acc, accuracy)
        total_error_rate <- c(total_error_rate, error_rate)
        total_AUC <- c(total_AUC, AUC)
    }
    total_acc_list <- c(total_acc_list, sum(total_acc) / k_fold)
    total_error_rate_list <- c(total_error_rate_list, sum(total_error_rate) / k_fold)
    total_AUC_list <- c(total_AUC_list, sum(total_AUC) / k_fold)
    report <- data.frame(Fold = c(1:10), Accuracy = total_acc, Error_rate = total_error_rate,
                         AUC = total_AUC)
    print(report)
    cat("==========================================================\n")
    print(paste("Overall accuracy is", round(sum(total_acc) / k_fold, 2), sep = " "))
    print(paste("Overall error is", round(sum(total_error_rate) / k_fold, 2), sep =" "))
    print(paste("Overall area under the curve(AUC) is", round(sum(total_AUC) / k_fold, 2), sep = " "))
    cat("\n==========================================================\n")
}

max_acc_index <- which.max(total_acc_list)
best_cost <- cost_values[max_acc_index]
best_acc <- total_acc_list[max_acc_index]
best_error_rate <- total_error_rate_list[max_acc_index]
best_AUC <- total_AUC_list[max_acc_index]

print(paste("Best model is one with cost as", best_cost, sep = " "))
print(paste("Best model accuracy is", round(best_acc, 2), sep = " "))
print(paste("Best model error is", round(best_error_rate, 2), sep =" "))
print(paste("Best model area under the curve(AUC) is", round(best_AUC, 2), sep = " "))
```


#### Question 8i

(5) points (bonus)) Re-run the analysis in (c) using a nested cross-validation to determine the best value of k and to determine the best parameters of the SVM.

```{r question8i}
# Setting the k values for KNN
k_values <- c(1, 3, 5, 7, 9)

# Intializing the variables
max_accuracy <- 0
max_k <- 0

# Running the KNN for the mentioned K values
# with 10 - fold cross validation
# Accuracy, error, AUC are rounded off to two decimal points
for(k in k_values) {
    total_acc <- c()
    total_error_rate <- c()
    total_AUC <- c()
    for(fold in 1:k_fold) {
        train_set <- music[-cv_index_list[[k_fold]], ]
        test_set <- music[cv_index_list[[k_fold]], ]
        knn_predicted_values <- knn(train = train_set, test = test_set, cl = train_set$Top10, k = k)
        accuracy <- round(Accuracy(knn_predicted_values, test_set$Top10), 2)
        error_rate <- 1 - accuracy
        AUC <- round(AUC(knn_predicted_values, test_set$Top10), 2)
        total_acc <- c(total_acc, accuracy)
    }
    current_accuracy <- sum(total_acc) / k_fold
    if(current_accuracy > max_accuracy) {
        max_accuracy <- current_accuracy
        max_k <- k
    }

}
print(paste("The best value of k is", max_k, sep = " "))


# Intializing the cost variables
cost_values <- c(0.01, 0.1, 1, 10, 100)
total_acc_list <- c()
total_error_rate_list <- c()
total_AUC_list <- c()

# Running the Support Vector machine for cost values
# 0.01, 0.1, 1, 10, 10, 100 with 10 - fold cross validation
# Accuracy, error, AUC are rounded off to two decimal points
for(cost in cost_values) {

    # Intailize variables
    total_acc <- c()
    total_error_rate <- c()
    total_AUC <- c()

    for(fold in 1:k_fold) {
        train_set <- music[-cv_index_list[[fold]], ]
        test_set <- music[cv_index_list[[fold]], ]

        # Fitting the model
        svm_fit <- svm(as.factor(Top10) ~ ., data = train_set, kernel = "radial", cost = cost)

        # Calculating Accuracy. Error, AUC
        test_prediction <- predict(svm_fit, test_set, type = 'class')
        accuracy <- round(Accuracy(test_prediction, test_set$Top10), 2)
        error_rate <- 1 - accuracy
        AUC <- round(AUC(test_prediction, test_set$Top10), 2)
        total_acc <- c(total_acc, accuracy)
        total_error_rate <- c(total_error_rate, error_rate)
        total_AUC <- c(total_AUC, AUC)
    }
    total_acc_list <- c(total_acc_list, sum(total_acc) / k_fold)
    total_error_rate_list <- c(total_error_rate_list, sum(total_error_rate) / k_fold)
    total_AUC_list <- c(total_AUC_list, sum(total_AUC) / k_fold)
}

max_acc_index <- which.max(total_acc_list)
best_cost <- cost_values[max_acc_index]
best_acc <- total_acc_list[max_acc_index]
best_error_rate <- total_error_rate_list[max_acc_index]
best_AUC <- total_AUC_list[max_acc_index]


print(paste("Best model is one with cost as", best_cost, sep = " "))
print(paste("Best model accuracy is", round(best_acc, 2), sep = " "))
print(paste("Best model error is", round(best_error_rate, 2), sep =" "))
print(paste("Best model area under the curve(AUC) is", round(best_AUC, 2), sep = " "))
```


#### Classification of Spam

#### Question 1

For this problem, you will work to classify e-mail messages as spam or not. The data set to be used is given with the project (note, this is not the same spam data set that is available from UCI ML repository, the data set is on Canvas and was also used in P3).

#### Question1a

Load in the spam data. You should not include the following columns in the classification task: isuid, id, domain, spampct, category, and cappct.

```{r Question1a}
# Loading the dataset
spam_dataset <- read.csv('data/spam.csv')

# Removing some of the columns
spam_dataset <- spam_dataset[c(-1,-2,-7,-11,-19,-20)]

# Checking for NA values in the data
sapply(spam_dataset, function(x) sum(is.na(x)))
```


#### Question1b

(4 points) Split the data into a training and test set with an 80/20 split of the data.

```{r Question1b}
# Setting the seed
set.seed(123)

# Random Split index variable
split <- sample.split(Y = spam_dataset$spam,SplitRatio = 80/100)

# Preparing Train and Test
train_spam <- subset(x = spam_dataset,split == TRUE)
test_spam <- subset(x = spam_dataset,split == FALSE)
```


#### Question1c

(8 points) Use a Naive Bayes classifier to predict spam. Calculate and report the accuracy, error, and AUC performance on the testing data.

```{r Question1c}
# Training the model
nbayes <- naiveBayes(x = train_spam[-15], y = train_spam$spam)

# Predicting the test set on the trained model
nb_pred <- predict(object = nbayes, newdata = test_spam[-15])

# Function to report accuracy, error and AUC
report <- function(test, pred) {
    
    # Confusion Matrix
    conf <- confusionMatrix(test$spam, pred)
    print(paste0("Accuracy is ", round(conf$overall[["Accuracy"]], 2)))
    print(paste0("Error rate is ", round(1-conf$overall[["Accuracy"]], 2)))
    
    # ROC Curve
    prediction <- prediction(as.numeric(nb_pred), as.numeric(test$spam))
    roc_perf <- performance(prediction, "tpr", "fpr")
    print(paste0("AUC value is ", round(auc(as.numeric(test$spam), as.numeric(pred)), 2)))
    plot(roc_perf, xlab="False positive rate", ylab="True positive rate")
    
}

# Calling the report
report(test_spam, nb_pred)
```


#### Question1d

(8 points) Learn a decision tree to predict spam. Calculate and report the accuracy, error, and AUC performance on the testing data.

```{r Question1d}
# Training the model
dtree <- rpart(formula = spam ~., data = train_spam)

# Predicting the test set on the trained model
dtree_pred <- predict(object = dtree, newdata = test_spam[-15], type = 'class')

# Calling the report
report(test_spam, dtree_pred)
```


#### Question1e

(10 points) Use Random Forests to predict spam. Calculate and report the accuracy, error, and AUC performance on the testing data.

```{r Question1e}
# Training the model
rand <- randomForest(x = train_spam[-15], y = train_spam$spam)

# Predicting the test set on the trained model
rand_pred <- predict(object = rand, newdata = test_spam[-15])

# Calling the report
report(test_spam, rand_pred)
```


#### Question1f

(12 points) Learn a support vector machine (SVM) with a RBF kernel to predict spam. Use cross-validation to pick the best parameters for the kernel and SVM on the training set. Consider at least the following values for cost: {0:01; 0:1; 1; 10; 100} Calculate and report the accuracy, error, and AUC performance on the testing data for the best model found.

```{r Question1f}
# Intializing the cost variables
cost_values <- c(0.01, 0.1, 1, 10, 100)

# Intailize variables
total_acc <- c()
total_error_rate <- c()
total_AUC <- c()

# Running the Support Vector machine for cost values
# 0.01, 0.1, 1, 10, 10, 100 with 10 - fold cross validation
# Accuracy, error, AUC are rounded off to two decimal points
for(cost in cost_values) {

    # Fitting the model
    svm_fit <- svm(as.factor(spam) ~ ., data = train_spam, kernel = "radial", cost = cost)

    # Calculating Accuracy. Error, AUC
    test_prediction <- predict(svm_fit, test_spam, type = 'class')
    accuracy <- round(Accuracy(test_prediction, test_spam$spam), 2)
    error_rate <- 1 - accuracy
    pred <- revalue(test_prediction, c("no" = 0, "yes" = 1))
    actual <- revalue(test_spam$spam, c("no" = 0, "yes" = 1))
    AUC <- round(AUC(pred, actual), 2)
    total_acc <- c(total_acc, accuracy)
    total_error_rate <- c(total_error_rate, error_rate)
    total_AUC <- c(total_AUC, AUC)
}
report <- data.frame(Cost_Values = cost_values, Accuracy = total_acc, Error_rate = total_error_rate,
                     AUC = total_AUC)
print(report)

max_acc_index <- which.max(total_acc)
best_cost <- cost_values[max_acc_index]
best_acc <- total_acc[max_acc_index]
best_error_rate <- total_error_rate[max_acc_index]
best_AUC <- total_AUC[max_acc_index]

print(paste("Best model is one with cost as", best_cost, sep = " "))
print(paste("Best model accuracy is", round(best_acc, 2), sep = " "))
print(paste("Best model error is", round(best_error_rate, 2), sep =" "))
print(paste("Best model area under the curve(AUC) is", round(best_AUC, 2), sep = " "))

# Taking away the best model(cost is 1)
svm_fit <- svm(as.factor(spam) ~ ., data = train_spam, kernel = "radial", cost = 1)
svm_pred <- predict(svm_fit, test_spam, type = 'class')
```


#### Question1g

(12 points) Examine which samples are mis-classified by some of the above models (construct a matrix with a column of predictions for each method: Naive Bayes, Decision Trees, SVMs). Create an ensemble predictor that takes the majority vote of the three models. Calculate and report the accuracy, error, and AUC performance on the testing data.

```{r Question1g}
# Doing the ensemble method
pred_df <- data.frame(Naive_Bayes = nb_pred, Decision_Tree = dtree_pred, SVM = svm_pred)
pred_df_numeric <- data.frame(revalue(nb_pred, c("no" = 0, "yes" = 1)), 
                              revalue(dtree_pred, c("no" = 0, "yes" = 1)),
                              revalue(svm_pred, c("no" = 0, "yes" = 1)))
spam <- rowMeans(apply(pred_df_numeric, 2, function(x) as.numeric(x)))
pred_df$spam <- ifelse(spam > .5, "yes", "no")

# Calculating accuracy, error, and AUC
accuracy <- round(Accuracy(pred_df$spam, test_spam$spam), 2)
error_rate <- 1 - accuracy
pred <- revalue(test_prediction, c("no" = 0, "yes" = 1))
actual <- revalue(test_spam$spam, c("no" = 0, "yes" = 1))
AUC <- round(AUC(pred, actual), 2)

# Reporting
print(paste0("Accuracy is ", round(accuracy, 2)))
print(paste0("Error rate is ", round(error_rate, 2)))
print(paste0("AUC is ", round(AUC, 2)))
```


#### Question1h

(8 points) Use bagging to fit an ensemble of 100 trees to the training data. Report the error rate on the testing data. Hint: In R, the package adabag includes methods for bagging and boosting of trees, where command bagging could also be used; other packages exist. In Matlab, look for the TreeBagger functions.

```{r Question1h}
# Training the model
bag <- bagging(formula = spam ~., data = train_spam, mfinal = 100)

# Predicting the test set on the trained model
bag_pred <- predict(object = bag, newdata = test_spam[-15])

# Finding the error rate
conf <- confusionMatrix(test_spam$spam, bag_pred$class)
print(paste0("The error rate for bagging is ", round(1 - conf$overall[["Accuracy"]], 2)))
```


#### Question1i

(8 points) Use boosting to fit an ensemble of 100 trees to the training data. Report the error rate on the testing data. Hint: In R, consider using the adaboost.M1 function (other packages include ada, adaStump, etc.). In Matlab, consider the fitcensemble function.

```{r Question1i}
# Training the model
boost <- boosting(formula = spam ~., data = train_spam, mfinal = 100)

# Predicting the test set on the trained model
boost_pred <- predict(object = boost,newdata = test_spam[-15])

# Finding the error rate
conf <- confusionMatrix(test_spam$spam, boost_pred$class)
print(paste0("The error rate for bagging is ", round(1 - conf$overall[["Accuracy"]], 2)))
```


#### Naive Bayes Classification

#### Question2

Fruit - Naive Bayes Classification Consider the fruit data set, fruit.txt. You will constructand evaluate a Naive Bayes classfier to predict whether a fruit is an apple or an orange using the remaining features (weight, height, and width). Please refer to the file fruit info.txt to understand the attributes and classes of the data.

#### Question2a

(5 points) Split the data into training and testing data using a 75/25 split.

```{r Question2a, error=FALSE}
# Reading the dataset
fruit_dataset <- read.csv("data/fruit.txt", header = FALSE)
names(fruit_dataset) <- c("Type", "Weight", "Height", "Width")

# For laplace we need factors
fruit_dataset <- as.data.frame(lapply(fruit_dataset, factor))

# Checking for NA values in the data
sapply(fruit_dataset, function(x) sum(is.na(x)))

# Setting the seed
set.seed(123)

# Random Split index variable
split <- sample.split(Y = fruit_dataset$Type, SplitRatio = 75/100)

# Preparing Train and Test
train_fruit <- subset(x = fruit_dataset, split == TRUE)
test_fruit <- subset(x = fruit_dataset, split == FALSE)
```


#### Question2b

(5 points) Estimate the probabilities needed for Naive Bayes classification using Laplace smoothing, e.g., P(Xi = j j Y = yk) = #(Xi=j;Y =yk)+1 / #(Y =yk)+jdomain(Xi)j

```{r Question2b}
# Naive bayes classifcation function with laplace smoothing

nbClassifier <- function(response, predictors, laplace = 0) {
    
    # Intailize variables
    response_name <- deparse(substitute(response))
    predictors <- as.data.frame(predictors)
    
    # Create Prior probablities
    # Percentage of each levels in response
    priorProb <- table(response) / length(response)
    
    # Conditional probablity using laplace formula
    condProb <- lapply(predictors, function(x) {
        tab <- table(response, x)
        (tab + laplace) / (rowSums(tab) + laplace * nlevels(x))
    })
    
    # Fix dimname names for conditional and prior probs
    for (i in 1:length(condProb)) {
        names(dimnames(condProb[[i]])) <- c(response_name, colnames(predictors)[i])
    }
    names(dimnames(priorProb)) <- response_name
    
    # Class Labels for the response
    if(is.logical(response)) 
        levels <- c(FALSE, TRUE) 
    else 
        levels <- levels(response)
    
    list(priorProb = priorProb, condProb = condProb, levels = levels)
}

nb_fit <- nbClassifier(train_fruit$Type, train_fruit[,-1], 1)
nb_fit
```


#### Question2c

(12 points) Report the predicted class on the test samples using the estimated parameters. Do these calculation using code/functions that you create and the probabilities estimated in part (b); do not use a supplied function/package in R, Matlab, or Python.

```{r Question2c}
# Checking whether the test varaibles are numic or logical 
isnumeric <- sapply(test_fruit, is.numeric)
islogical <- sapply(test_fruit, is.logical)
    
# Probality computation for the test set
probCompute <- function(v, ndata, predictors, model) {
            
    nd <- ndata[predictors[v]]
    if(is.na(nd)) {
        rep(1, length(model$priorProb)) 
    }
    else {
        prob <- if (isnumeric[predictors[v]]) {
            predCondProb <- model$condProb[[v]]
            print(msd)
            predCondProb[, 2][predCondProb[, 2] <= 0] <- 0.001
            dnorm(nd, msd[, 1], msd[, 2])
        } else {
            model$condProb[[v]][, nd + islogical[predictors[v]]]
        }
        prob[prob <= 0] <- 0.001
        prob
    }
}

predictNaiveBayes <- function(model, test) {
    
    predictors <- match(names(model$condProb), names(test))
    test <- data.matrix(test)
    
    resultProb <- sapply(1:nrow(test), function(i) {
        ndata <- test[i, ]
        log(model$priorProb) + apply(log(sapply(seq(predictors), function(v) probCompute(v, ndata, predictors, model))), 1, sum)
    })
    
    if (is.logical(model$levels))
        resultProb[2,] > resultProb[1,]
    else
        factor(model$levels[apply(resultProb, 2, which.max)], levels = model$levels)
    
}

predictNaiveBayes(nb_fit, test_fruit)

```


#### Question2d

(2 points) Confirm the results above using a Naive Bayes classifier available a function or package.

```{r Question2d}
nb_model_fit <- naiveBayes(Type ~ ., train_fruit, laplace = 1)
predict(nb_model_fit, test_fruit, type = "class")
```


#### Question2e

(6 points) Repeat the evaluation using a 75/25 split 10 times. Report the accuracy, sensitivity, and specificity for each of the 10 repetitions and averaged over the 10 repetitions.

```{r Question2e}
# Setting the seed for reproduciablility
set.seed(294)

# NO of fold
k_fold <- 10

# Creating the folds variable in the data frame
cv_index_list <- createFolds(fruit_dataset$Type, k = k_fold, list = TRUE, returnTrain = FALSE)

# Checking the class label proportions
for(i in 1:k_fold) {
    fold <- fruit_dataset$Type[cv_index_list[[i]]]
    fold_length <- length(fold)
    class1_prop <- round(length(fold[fold == 1]) / fold_length, 3)
    class2_prop <- round(length(fold[fold == 2]) /  fold_length, 3)
    class3_prop <- round(length(fold[fold == 3]) /  fold_length, 3)
    print(paste("Fold" , i , "length", fold_length, sep = " "))
    print(paste("Class Label(1)", round(class1_prop, 3), sep = " "))
    print(paste("Class Label(2)", round(class2_prop, 3), sep = " "))
    print(paste("Class Label(3)", round(class3_prop, 3), sep = " "))
}

# Function to calculate sensitivity
mySensitivity <- function(matrix) {
    value <- c()
    seq <- 1:nrow(matrix)
    for(i in seq) {
        den_seq <- seq[seq != i]
        B <- matrix[den_seq[1], i] + matrix[den_seq[2], i]
        value <- c(value, matrix[i, i] / (matrix[i, i] + B))
    }
    value
}


# Function to calculate sensitivity
mySpecificity <- function(matrix) {
    value <- c()
    seq <- 1:nrow(matrix)
    for(i in seq) {
        den_seq <- seq[seq != i]
        TN <- matrix[den_seq[1], den_seq[1]] + matrix[den_seq[2], den_seq[2]]
        FP <- matrix[i, den_seq[1]] + matrix[i, den_seq[2]]
        value <- c(value, TN / (TN + FP))
    }
    value
}

# Intailize variables
total_acc <- c()
class_1_sensitivity <- c()
class_2_sensitivity <- c()
class_3_sensitivity <- c()
class_1_specificity <- c()
class_2_specificity <- c()
class_3_specificity <- c()

# Running the Naive bayes with 10 - fold cross validation
# Accuracy, error, AUC are rounded off to two decimal points
naiveBayesModel <- function() {
    for(fold in 1:k_fold) {
        train_set <- fruit_dataset[-cv_index_list[[fold]], ] 
        test_set <- fruit_dataset[cv_index_list[[fold]], ] 
        
        # Fitting the Naive bayes model
        nb_fit <- naiveBayes(as.factor(Type) ~ ., data = train_set, laplace = 1)
        
        # Calculating Accuracy. Sensitivity, Specificity
        test_prediction <- predict(nb_fit, test_set, type = "class")
        type <- as.numeric(test_set$Type)
        
        accuracy <- round(Accuracy(test_prediction, test_set$Type), 2)
        total_acc <- c(total_acc, accuracy)
        
        conf_matrix <- table(factor(test_prediction, levels=min(type):max(type)), 
                             factor(test_set$Type, levels = min(type):max(type)))
        sensitivity <- mySensitivity(conf_matrix)
        class_1_sensitivity <- c(class_1_sensitivity, sensitivity[1])
        class_2_sensitivity <- c(class_2_sensitivity, sensitivity[2])
        class_3_sensitivity <- c(class_3_sensitivity, sensitivity[3])
        
        specificity <- mySpecificity(conf_matrix)
        class_1_specificity <- c(class_1_specificity, specificity[1])
        class_2_specificity <- c(class_2_specificity, specificity[2])
        class_3_specificity <- c(class_3_specificity, specificity[3])
    }
    
    report <- data.frame(Fold = c(1:10), Accuracy = total_acc, Class_1_Sensitivity = class_1_sensitivity, Class_2_Sensitivity = class_2_sensitivity, Class_3_Sensitivity = class_3_sensitivity, Class_1_Specificity = class_1_specificity, Class_2_Specificity = class_2_specificity, Class_2_Specificity = class_3_specificity)
    print(report)
    print("Overall")
    print(paste("Accuracy is", round(sum(total_acc) / k_fold, 2), sep = " "))
    print(paste("Class 1 Sensitivity is", round(sum(class_1_sensitivity) / k_fold, 2), sep = " "))
    print(paste("Class 2 Sensitivity is", round(sum(class_2_sensitivity) / k_fold, 2), sep = " "))
    print(paste("Class 3 Sensitivity is", round(sum(class_3_sensitivity) / k_fold, 2), sep = " "))
    print(paste("Class 1 Specificity is", round(sum(class_1_specificity) / k_fold, 2), sep = " "))
    print(paste("Class 2 Specificity is", round(sum(class_2_specificity) / k_fold, 2), sep = " "))
    print(paste("Class 3 Specificity is", round(sum(class_3_specificity) / k_fold, 2), sep = " "))
}
naiveBayesModel()
```


#### Text Classifiction

#### Question3

For this question you will be considering text classification using two different Naive Bayes
models. These approaches will be discussed in class and reference the following book.

Manning, C., Raghavan, P., Schutze, H. Introduction to Information Retrieval, Cambridge University Press, 2008 

<http://nlp.stanford.edu/IR-book/>

In particular, look at Chapter 13 

<http://nlp.stanford.edu/IR-book/html/htmledition/text-classification-and-naive-bayes-1.html>

You will be using data from the Presidential State of the Union Addresses available as a zip archive. The speeches are available in text files sorted by year, e.g., a1.txt, ..., a231.txt. The text files are formated such that there is one word per line and most punctuation has been removed (note, there are still hyphens or dashes left in the text files).

#### Question 3a

(4 points) Load the addresses. You will need to create a vector listing the party affliation of each president to match their speech, you may use the file party.txt to help with this classification.

```{r Question3a}
# Loading the addresses
speeches <- DirSource("data/sotu/files/")
speech_corpus <- Corpus(speeches, readerControl = list(language="en", stopwords=FALSE,
                                                       wordLengths=c(0,Inf)))

# Read the party.txt file
party_df <- read.csv("data/sotu/party.txt", header = FALSE, col.names = c("colparty", "colpresident", "colyear"))
# Subset the party dataframe
train_party <- subset(party_df, !(party_df$colyear %in% c(2017, 2014, 2006, 1995, 1962)))
test_party <- subset(party_df, party_df$colyear %in% c(2017, 2014, 2006, 1995, 1962))
```


#### Question 3b

(6 points) Remove stopwords from consideration for the method. The stopwords are available at stopwords.txt.


```{r Question3b}
# Loading the stopwords.txt
stopwords <- read.csv("data/sotu/stopwords.txt", header = FALSE, col.names = c("words"))

# Removing Stopword from the considerations
corpus_after_stopwords <- tm_map(speech_corpus, removeWords, stopwords$words)
```

#### Question 3c

Predict the party affliations (Democrat / Republican) for the following speeches:

* Donald Trump, 2017
* Barack Obama, 2014
* George W. Bush, 2006
* William Clinton, 1995
* John F. Kennedy, 1962

The training set will be the remaining speeches that can be associated with the Democratic or Republican presidents (note, you will not need all the addresses, but they were included here for completeness of the data). You will need to complete the following steps:

#### Question 3ci

(10 points) Create a term-document matrix, TD for this set of speeches.


```{r Question3ci}
# Preparing Document Matrix
dtm = DocumentTermMatrix(corpus_after_stopwords, control = list(tolower = TRUE,
                                                                removePunctuation = TRUE, 
                                               removeNumbers = TRUE, stopwords = FALSE, 
                                               wordLengths = c(1,Inf)))
DT <- as.matrix(dtm)
doc_name <- rownames(DT)
file_num <- substring(doc_name, 2, nchar(doc_name) - 4)
DT <- DT[order(as.numeric(file_num)), ]

#Binding the party details to the Corpus matrix
DT <- cbind(DT, colparty = party_df$colparty, colyear = party_df$colyear)

train_DT <- subset(DT, !(DT[, ncol(DT)] %in% c(2017, 2014, 2006, 1995, 1962)))
train_DTSince1913 <- subset(DT, !(DT[, ncol(DT)] %in% c(2017, 2014, 2006, 1995, 1962)) & DT[, ncol(DT)] > 1912)
test_DT <- subset(DT, DT[, ncol(DT)] %in% c(2017, 2014, 2006, 1995, 1962))
```


#### Question 3cii

(30 points) For the 5 speeches listed above determine the party affliations of the president. Calculate and report P(C = Dems jX) and P(C = Reb jX) under the Bernoulli model of Naive Bayes. In order to avoid underflow errors, use the log probabilities as discussed in class. Note, for this question and the next you will not use the standard Naive Bayes package, library or function; rather you will implement the method as described in pseudocode of the IR book.


```{r Question3cii}
# Preprocessing
train_DF <- data.frame(train_DT)
party_train <- subset(party_df, !(party_df[, ncol(party_df)] %in% c(2017, 2014, 2006, 1995, 1962)))
train_DF <- cbind(party_train$colparty,train_DF)

train_Dem <- train_DF[train_DF$`party_train$colparty`=='d',]
train_Dem$`party_train$colparty` <- NULL

train_Rep <- train_DF[train_DF$`party_train$colparty`=='r',]
train_Rep$`party_train$colparty` <- NULL

# Prior Probabilities of each class
p_dem <- nrow(train_Dem)/nrow(train_DF)
p_rep <- nrow(train_Rep)/nrow(train_DF)

# Bernoulli Calculation
train_bern_dem <- train_Dem
train_bern_rep <- train_Rep
train_bern_dem[train_bern_dem != 0] <- 1
train_bern_rep[train_bern_rep != 0] <- 1
prob_bern_dem <- data.frame((colSums(train_bern_dem)+1)/(nrow(train_bern_dem)+2))
prob_bern_rep <- data.frame((colSums(train_bern_rep)+1)/(nrow(train_bern_rep)+2))

# Changing test data matrix to data frame
test_df <- data.frame(test_DT)
test_df <- data.frame(t(test_df))

# Predicting the party affiliation using bernoulli
prob_speeches_bern <- function(x){
    a_pred_dem <- cbind(prob_bern_dem,test_df[,x])
    colnames(a_pred_dem) <- c('probability','occurance')
    a_pred_dem$probability <- log(a_pred_dem$probability)
    a_pred_dem <- mutate(.data = a_pred_dem,res = a_pred_dem$probability*a_pred_dem$occurance)
    cond_dem <- log(p_dem)+sum(a_pred_dem$res)
    
    a_pred_rep <- cbind(prob_bern_rep,test_df[,x])
    colnames(a_pred_rep) <- c('probability','occurance')
    a_pred_rep$probability <- log(a_pred_rep$probability)
    a_pred_rep <- mutate(.data = a_pred_rep,res = a_pred_rep$probability*a_pred_rep$occurance)
    cond_rep <- log(p_rep)+sum(a_pred_rep$res)
    
    return(c(cond_dem,cond_rep))
}

kennedy <- prob_speeches_bern(1)
kennedy <- c(kennedy, ifelse(kennedy[1] > kennedy[2], "D", "R"))
clinton <- prob_speeches_bern(2)
clinton <- c(clinton, ifelse(clinton[1] > clinton[2], "D", "R"))
bush <- prob_speeches_bern(3)
bush <- c(bush, ifelse(bush[1] > bush[2], "D", "R"))
obama <- prob_speeches_bern(4)
obama <- c(obama, ifelse(obama[1] > obama[2], "D", "R"))
trump <- prob_speeches_bern(5)
trump <- c(trump, ifelse(trump[1] > trump[2], "D", "R"))

data.frame(party = c("D", "R", "Prediction"), kennedy, clinton, bush, obama, trump)
```

**The classifier assigns the document to the party which has highest P(Class | Document) value.**

#### Question 3ciii

(30 points) For the 4 speeches listed above determine the party affliations of the president. Calculate and report P(C = Dems jX) and P(C = Reb jX) under the Multinomial model of Naive Bayes.

Hint: A strong suggestion for this question is to first create and test your code on a small document set, e.g., the example in the IR-book. Once you have that working correctly, then run on the SOTU addresses.

```{r Question3ciii}
# Multinomial
prob_multi_dem <- data.frame((colSums(train_Dem)+ 1) / (sum(train_Dem)+ncol(train_Dem)))
prob_multi_rep <- data.frame((colSums(train_Rep)+ 1) / (sum(train_Rep)+ncol(train_Rep)))

prob_speeches_multi <- function(x){
    
    a_pred_dem <- cbind(prob_multi_dem,test_df[,x])
    colnames(a_pred_dem) <- c('probability','occurance')
    a_pred_dem$probability <- log(a_pred_dem$probability)
    a_pred_dem <- mutate(.data = a_pred_dem,res = a_pred_dem$probability*a_pred_dem$occurance)
    cond_dem <- log(p_dem)+sum(a_pred_dem$res)
    
    a_pred_rep <- cbind(prob_multi_rep,test_df[,x])
    colnames(a_pred_rep) <- c('probability','occurance')
    a_pred_rep$probability <- log(a_pred_rep$probability)
    a_pred_rep <- mutate(.data = a_pred_rep,res = a_pred_rep$probability*a_pred_rep$occurance)
    cond_rep <- log(p_rep)+sum(a_pred_rep$res)
    
    return(c(cond_dem,cond_rep))
}

# Calling the predict function
kennedy <- prob_speeches_multi(1)
kennedy <- c(kennedy, ifelse(kennedy[1] > kennedy[2], "D", "R"))
clinton <- prob_speeches_multi(2)
clinton <- c(clinton, ifelse(clinton[1] > clinton[2], "D", "R"))
bush <- prob_speeches_multi(3)
bush <- c(bush, ifelse(bush[1] > bush[2], "D", "R"))
obama <- prob_speeches_multi(4)
obama <- c(obama, ifelse(obama[1] > obama[2], "D", "R"))
trump <- prob_speeches_multi(5)
trump <- c(trump, ifelse(trump[1] > trump[2], "D", "R"))

data.frame(party = c("D", "R", "Prediction"), kennedy, clinton, bush, obama, trump)
```

**The classifier assigns the document to the party which has highest P(Class | Document) value.**

#### Question 3d

(4 points (bonus)) Repeat the prediction of the party affliation for the five speeches listed
using only the other speeches since 1913 as the training data.

```{r Question3d bernoulli}
# Preprocessing
train_DF <- data.frame(train_DTSince1913)
party_train <- subset(party_df, !(party_df[, ncol(party_df)] %in% c(2017, 2014, 2006, 1995, 1962)))
party_train <-  subset(party_train, party_train[, ncol(party_train)] > 1912)
train_DF <- cbind(party_train$colparty,train_DF)

train_Dem <- train_DF[train_DF$`party_train$colparty`=='d',]
train_Dem$`party_train$colparty` <- NULL

train_Rep <- train_DF[train_DF$`party_train$colparty`=='r',]
train_Rep$`party_train$colparty` <- NULL

# Prior Probabilities of each class
p_dem <- nrow(train_Dem)/nrow(train_DF)
p_rep <- nrow(train_Rep)/nrow(train_DF)

# Bernoulli Calculation
train_bern_dem <- train_Dem
train_bern_rep <- train_Rep
train_bern_dem[train_bern_dem != 0] <- 1
train_bern_rep[train_bern_rep != 0] <- 1
prob_bern_dem <- data.frame((colSums(train_bern_dem)+1)/(nrow(train_bern_dem)+2))
prob_bern_rep <- data.frame((colSums(train_bern_rep)+1)/(nrow(train_bern_rep)+2))

# Changing test data matrix to data frame
test_df <- data.frame(test_DT)
test_df <- data.frame(t(test_df))

# calling the predict function
kennedy <- prob_speeches_bern(1)
kennedy <- c(kennedy, ifelse(kennedy[1] > kennedy[2], "D", "R"))
clinton <- prob_speeches_bern(2)
clinton <- c(clinton, ifelse(clinton[1] > clinton[2], "D", "R"))
bush <- prob_speeches_bern(3)
bush <- c(bush, ifelse(bush[1] > bush[2], "D", "R"))
obama <- prob_speeches_bern(4)
obama <- c(obama, ifelse(obama[1] > obama[2], "D", "R"))
trump <- prob_speeches_bern(5)
trump <- c(trump, ifelse(trump[1] > trump[2], "D", "R"))

data.frame(party = c("D", "R", "Prediction"), kennedy, clinton, bush, obama, trump)
```

**The classifier assigns the document to the party which has highest P(Class | Document) value.**

```{r Multinomial}
# Multinomial
prob_multi_dem <- data.frame((colSums(train_Dem)+ 1) / (sum(train_Dem)+ncol(train_Dem)))
prob_multi_rep <- data.frame((colSums(train_Rep)+ 1) / (sum(train_Rep)+ncol(train_Rep)))

kennedy <- prob_speeches_multi(1)
kennedy <- c(kennedy, ifelse(kennedy[1] > kennedy[2], "D", "R"))
clinton <- prob_speeches_multi(2)
clinton <- c(clinton, ifelse(clinton[1] > clinton[2], "D", "R"))
bush <- prob_speeches_multi(3)
bush <- c(bush, ifelse(bush[1] > bush[2], "D", "R"))
obama <- prob_speeches_multi(4)
obama <- c(obama, ifelse(obama[1] > obama[2], "D", "R"))
trump <- prob_speeches_multi(5)
trump <- c(trump, ifelse(trump[1] > trump[2], "D", "R"))

data.frame(party = c("D", "R", "Prediction"), kennedy, clinton, bush, obama, trump)
```

**The classifier assigns the document to the party which has highest P(Class | Document) value.**

#### Question 4

(4 points (bonus)) Look at additional analysis and visualization of the State of the Union Addresses (more complex examples will receive the full amount of bonus points). For example, 

* plot the number of unique words and total words in the addresses,
* visualize the frequency of certain words or phrases in the addresses, etc.


```{r Question4}
# Term Frequency
freq <- colSums(as.matrix(dtm))
plot(sort(freq, decreasing = T),col="blue",main="Word frequencies", xlab="Words", ylab = "TF")

# Top 10 Words 
high.freq=tail(sort(freq),n=10)
hfp.df=as.data.frame(sort(high.freq))
hfp.df$names <- rownames(hfp.df)
ggplot(hfp.df, aes(reorder(names,high.freq), high.freq)) +
  geom_bar(stat="identity") + coord_flip() + 
  xlab("Terms") + ylab("Frequency") +
  ggtitle("Term frequencies")
```


**End of the assignment**