---
title: "Project 3 Group 3"
author: "Nishanth Gandhidoss, Raghavendran Shankar"
date: "4 March 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r include=FALSE}
# installing the packages
# Function to Install packages
# checks the available.packages before
installNewPackage <- function(packageName) {
        if(packageName  %in% rownames(installed.packages()) == FALSE)
        {
                install.packages(packageName, repos = "http://cran.us.r-project.org", dependencies=TRUE)
        }
}

installNewPackage("ROCR")
installNewPackage("plyr")
installNewPackage("caret")
installNewPackage("pROC")
installNewPackage("rpart.plot")
# installNewPackage("cluster")
# installNewPackage("protoclust")
# installNewPackage("httr")
# installNewPackage("jsonlite")
# installNewPackage("httpuv")
# installNewPackage("curlconverter")

library(ROCR)
library(plyr)
library(caret)
library(ggplot2)
library(tidyr)
library(rpart)
library(caTools)
library(pROC)
library(rpart.plot)
# library(dplyr)
# library(devtools)
# library(ggbiplot)
# library(httr)
# library(jsonlite)
# library(httpuv)
# library(curlconverter)
# 
# # Comment it when your executing for second or the next time
# # for faster running 
# install_github("ggbiplot","vqv")
```

#### Question 1

(24 points) Write a function that will evaluate the predictions of a classifier. That is, given a vector of predicted values, Y~pred~, and actual class values, Y~true~, can report the following quantities:

* number of true positives (TP),
* number of false positives (FP),
* number of true negatives (TN),
* number of false negatives (FN),
* true positive rate (TPR),
* true negative rate (TNR),
* sensitivity (Sens.),
* specificity (Spec.),
* precision (Prec.),
* recall (Rec.),
* accuracy (Acc.), and
* error rate (Err)


```{r question1}
# Creting S4 object for the predcition evaluation
setClass("prediction", representation(predictions = "list", labels = "list", cutoffs = "list",
                                      fp = "list", tp = "list", tn = "list", fn = "list", 
                                      n.pos = "list", n.neg = "list", n.pos.pred = "list",
                                      n.neg.pred = "list", tpr = "numeric", fpr = "numeric", 
                                      tnr = "numeric", fnr = "numeric", sensitivity = "numeric",
                                      specificity = "numeric", precision = "numeric", 
                                      recall = "numeric", accuracy = "numeric", error_rate = "numeric"))

# Function to Evaluate the prediction
# Returns the necessary measures depeing 
# upon the passed argument
predictions <- function (predictions, labels, label.ordering = NULL) {
    # Check if data frame
    if (is.data.frame(predictions)) {
        names(predictions) <- c()
        predictions <- as.list(predictions)
    }
    # Check if it is matrix
    else if (is.matrix(predictions)) {
        predictions <- as.list(data.frame(predictions))
        names(predictions) <- c()
    }
    # Check if it is Vector
    else if (is.vector(predictions) && !is.list(predictions)) {
        predictions <- list(predictions)
    }
    # Stop if prediction is not of type list
    else if (!is.list(predictions)) {
        stop("Format of predictions is invalid.")
    }
    # Assign the labels
    if (is.data.frame(labels)) {
        names(labels) <- c()
        labels <- as.list(labels)
    }
    else if (is.matrix(labels)) {
        labels <- as.list(data.frame(labels))
        names(labels) <- c()
    }
    else if ((is.vector(labels) || is.ordered(labels) || is.factor(labels)) && 
        !is.list(labels)) {
        labels <- list(labels)
    }
    # Stop if labels is not of type list
    else if (!is.list(labels)) {
        stop("Format of labels is invalid.")
    }
    # Length check for single and multiple evaluations
    if (length(predictions) != length(labels)) 
        stop(paste("Number of cross-validation runs must be equal", 
            "for predictions and labels."))
    if (!all(sapply(predictions, length) == sapply(labels, length))) 
        stop(paste("Number of predictions in each run must be equal", 
            "to the number of labels for each run."))
    
    for (i in 1:length(predictions)) {
        finite.bool <- is.finite(predictions[[i]])
        predictions[[i]] <- predictions[[i]][finite.bool]
        labels[[i]] <- labels[[i]][finite.bool]
    }
    
    label.format = ""
    
    # Assign label fotmat for each type
    if (all(sapply(labels, is.factor)) && !any(sapply(labels, 
        is.ordered))) {
        label.format <- "factor"
    }
    else if (all(sapply(labels, is.ordered))) {
        label.format <- "ordered"
    }
    else if (all(sapply(labels, is.character)) || all(sapply(labels, 
        is.numeric)) || all(sapply(labels, is.logical))) {
        label.format <- "normal"
    }
    else {
        stop(paste("Inconsistent label data type across different", 
            "cross-validation runs."))
    }
    if (!all(sapply(labels, levels) == levels(labels[[1]]))) {
        stop(paste("Inconsistent factor levels across different", 
            "cross-validation runs."))
    }
    levels <- c()
    if (label.format == "ordered") {
        if (!is.null(label.ordering)) {
            stop(paste("'labels' is already ordered. No additional", 
                "'label.ordering' must be supplied."))
        }
        else {
            levels <- levels(labels[[1]])
        }
    }
    else {
        if (is.null(label.ordering)) {
            if (label.format == "factor") 
                levels <- sort(levels(labels[[1]]))
            else levels <- sort(unique(unlist(labels)))
        }
        else {
            if (!setequal(unique(unlist(labels)), label.ordering)) {
                stop("Label ordering does not match class labels.")
            }
            levels <- label.ordering
        }
        for (i in 1:length(labels)) {
            if (is.factor(labels)) 
                labels[[i]] <- ordered(as.character(labels[[i]]), 
                  levels = levels)
            else labels[[i]] <- ordered(labels[[i]], levels = levels)
        }
    }
    if (length(levels) != 2) {
        message <- paste("Number of classes is not equal to 2.\n", 
            "ROCR currently supports only evaluation of ", "binary classification tasks.", 
            sep = "")
        stop(message)
    }
    if (!is.numeric(unlist(predictions))) {
        stop("Currently, only continuous predictions are supported by ROCR.")
    }
    
    # Intailizing empty list
    cutoffs <- list()
    fp <- list()
    tp <- list()
    fn <- list()
    tn <- list()
    n.pos <- list()
    n.neg <- list()
    n.pos.pred <- list()
    n.neg.pred <- list()
    
    # Loop to perform the caluclation for evaluation metrics
    for (i in 1:length(predictions)) {
        n.pos <- c(n.pos, sum(labels[[i]] == levels[2]))
        n.neg <- c(n.neg, sum(labels[[i]] == levels[1]))
        ans <- .compute.unnormalized.roc.curve(predictions[[i]], 
            labels[[i]])
        cutoffs <- c(cutoffs, list(ans$cutoffs))
        fp <- c(fp, list(ans$fp))
        tp <- c(tp, list(ans$tp))
        fn <- c(fn, list(n.pos[[i]] - tp[[i]]))
        tn <- c(tn, list(n.neg[[i]] - fp[[i]]))
        n.pos.pred <- c(n.pos.pred, list(tp[[i]] + fp[[i]]))
        n.neg.pred <- c(n.neg.pred, list(tn[[i]] + fn[[i]]))
    }
    tpr <- tp[[1]] / max(tp[[1]])
    fpr <- fp[[1]] / max(fp[[1]])
    tnr <- tn[[1]] / max(tn[[1]])
    fnr <- fn[[1]] / max(fn[[1]])
    sensitivity <- tp[[1]] / (tp[[1]] + fn[[1]]) 
    specificity <- tn[[1]] / (fp[[1]] + tn[[1]])
    precision <- tp[[1]] / (tp[[1]] + fp[[1]])
    accuracy <- (tp[[1]] + tn[[1]]) / (tp[[1]] + fp[[1]] + fn[[1]] + tn[[1]])
    error_rate <- 1 - accuracy
    
    return(new("prediction", predictions = predictions, labels = labels, 
        cutoffs = cutoffs, fp = fp, tp = tp, fn = fn, tn = tn, 
        n.pos = n.pos, n.neg = n.neg, n.pos.pred = n.pos.pred, 
        n.neg.pred = n.neg.pred, tpr = tpr, fpr = fpr, tnr = tnr, fnr = fnr,
        sensitivity = sensitivity, specificity = specificity, precision = precision,
        recall = sensitivity, accuracy = accuracy, error_rate = error_rate))
}
```


#### Question 2

(20 points) For the following data set, you will compute the true positive rate, false positive rate, and accuracy (with the function you wrote for Prob. 1) using every predicted output as a possible threshold.


Sample  |   Y~true~   |   Y~Pred~
--------|-------------|------------
    1   |      1      |   0.98    
    2   |      0      |   0.92
    3   |      1      |   0.85
    4   |      1      |   0.77
    5   |      0      |   0.71
    6   |      0      |   0.64
    7   |      1      |   0.50
    8   |      0      |   0.39
    9   |      1      |   0.34
    10  |      0      |   0.31


```{r question2}
# Assigning the given data to varibles
y_true <- c(1, 0, 1, 1, 0 ,0, 1, 0, 1, 0)
y_pred <- c(0.98, 0.92, 0.85, 0.77, 0.71, 0.64, 0.50, 0.39, 0.34, 0.31)

# Calling the prediction function
pred <- predictions(y_pred, y_true)

# Creating dataframe for the required result
result_q2 <- data.frame(true.positve.rate = pred@tpr[-1], false.positive.rate = pred@fpr[-1],
                        accuracy = pred@accuracy[-1])

# Printing the reults
result_q2
```


#### Question 3

(10 points) Use the results from Prob. 2 to plot the ROC curve for the data. Note, plot this curve using the standard plotting tools rather than any special library/package available in R or Matlab.


```{r question3}
# Plotting the ROC curve for the data
plot(result_q2$false.positive.rate, result_q2$true.positve.rate, type = "l", col = "red")
abline(a=0, b=1)
```

#### Question 4

(12 points) Data Mining Book, 8.14.

Suppose that we want to select between two prediction models, M~1~ and M~2~. We have performed 10 rounds of 10-fold cross-validation on each model, where the same data partitioning in round i is used for both M~1~ and M~2~. The error rates obtained for M~1~ are 30.5, 32.2, 20.7, 20.6, 31.0, 41.0, 27.7, 26.0, 21.5, 26.0. The error rates for M~2~ are 22.4, 14.5, 22.4, 19.6, 20.7, 20.4, 22.1, 19.4, 16.2, 35.0. Comment on whether one model is significantly better than the other considering a significance level of 1%.


```{r question4}
paste0("Hypothesis testing can be done to see if there are significant difference in error rates. Since same data partitioning is used for both M1 and M2 i.e same test set, the paired observation hypothesis testing to compare means is
H0: me1 - me2 = 0
H1: me1 - me2 != 0 where me1 is the mean error of M1 and me2 is the mean error of M2")

M1 = c(30.5, 32.2, 20.7, 20.6, 31.0, 41.0, 27.7, 26.0, 21.5, 26.0)
M2 = c(22.4, 14.5, 22.4, 19.6, 20.7, 20.4, 22.1, 19.4, 16.2, 35.0)

t.test(M1,M2,paired = TRUE,conf.level = 0.99)
paste0("Since p-value is 0.0437 which is greater than the significance level of 0.01, we fail to reject the null hypothesis i.e the two models are not significantly different.")
```


#### Question 5

Suppose you are building a decision tree on a data set with three classes A;B;C. At the current position in the tree you have the following samples available:

$$
N = \left(\begin{array}{cc} 
A & 100\\
B & 50\\
C & 60\\
\end{array}\right)
$$ 


You are examining two ways to split the data. The first splits the data as,

$$
N~1,1~ = \left(\begin{array}{cc} 
A & 62\\
B & 8\\
C & 0\\
\end{array}\right)
$$
$$
N~1,2~ = \left(\begin{array}{cc} 
A & 38\\
B & 42\\
C & 60\\
\end{array}\right)
$$

The second splits the data as,

$$
N~2,1~ = \left(\begin{array}{cc} 
A & 65\\
B & 20\\
C & 0\\
\end{array}\right)
$$
$$
N~2,2~ = \left(\begin{array}{cc} 
A & 21\\
B & 19\\
C & 20\\
\end{array}\right)
$$
$$
N~2,3~ = \left(\begin{array}{cc} 
A & 14\\
B & 11\\
C & 40\\
\end{array}\right)
$$ 


#### Question 5a

(a) (18 points) Compute the gain in GINI index for the two splits. Show the form of the calculations, not just the final numbers.


```{r question5a}

```


#### Question 5b

(4 points) Which node would be preferred to include next in the decision tree?


```{r question5b}

```


#### Question 5c

(18 points) Compute the information gain (based on entropy) for the two splits. Show the form of the calculations, not just the final numbers.


```{r question5c}

```


#### Question 5d

(4 points) Which node would be preferred to include next in the decision tree?


```{r question5d}

```


#### Question 6

#### Classification of Age based on Social Media Usage

For this problem, you want to classify the age of an individual ("High School" or "Adult") basic on their social media app usage. The data was collected via a survey, with respondents consisting of the Women in Computing Sciences Summer Youth Program participants and female faculty at Michigan Tech. The data is available at syp-s16-data.csv. There are 60 responses with 26 Adults and 34 HS respondents.


#### Question 6a

(12 points) Create a small multiples plot showing the number of respondents who use or do not use each app grouped by age. Consider making grouped bar plots.


```{r question6a}
# Importing data
syp_16_data <-  read.csv('syp-16-data.csv')
head(syp_16_data)

# Converting data from wide to long format
data1 <- gather(data = syp_16_data,key = Network,value = vals,c(Facebook,Twitter,LinkedIn,Google.,Youtube,Pinterest,Instagram,Tumblr,Flickr,Snapchat,WhatsApp,Vine,Periscope,Viber,KikMessenger,Telegram,ooVoo,YikYak,Other),factor_key = TRUE)
# Finding the frequency
data1 <- data.frame(table(data1$Adult,data1$vals,data1$Network))
colnames(data1) <- c("Age","Value","Network","Count")
head(data1)

# Creating grouped barplots
ggplot(data = data1,aes(x = Age,y = Count))+geom_bar(aes(fill = Value),stat = "identity",position = "stack")+facet_grid(.~Network)

```


#### Question 6b

(4 points) From the figures in (a), which of the apps would you expect to find at the root of a deicision tree? that is, which app (variable) would be best to separate the two classes of responses?


```{r question6b}
paste0("The intuition is Facebook which could separate the two classes (Adults and HS).")
```


#### Question 6c

(10 points) Construct a decision tree using this data set. Does the variable at the root of the tree match the intuition from part (b)?


```{r question6c}
# Splitting the dataset as 75/25
set.seed(123)
split <- sample.split(syp_16_data$Adult,SplitRatio = 3/4)
training_set <- subset(syp_16_data,split == TRUE)
test_set <- subset(syp_16_data,split == FALSE)

# Fitting decision tree to training set
dtree <- rpart(formula = Adult~.,data = training_set)

# Predicting the output for test set
y_pred <- predict(object = dtree,newdata = test_set,type = 'class')

# Creating confusion matrix
cm <- confusionMatrix(test_set[,20],y_pred)
cm

# Plotting Decision tree
plot(dtree)
text(dtree)

paste0("The intuition was facebook but the variable at the root of the tree is Instagram")
```


#### Question 7

#### Classification of Spam: Trees

For this problem, you will work to classify e-mail messages as spam or not. The data set to be used is given with the project (note, this is not the same spam data set that is available from UCI ML repository).


#### Question 7a

Load in the spam data. You should not include the following columns in the classification task: isuid, id, domain, spampct, category, and cappct.


```{r question7a}
spam <- read.csv("spam.csv")
spam <- data.frame(spam[,-c(1,2,7,11,19,20)])
head(spam)
```


#### Question 7b

(4 points) Split the data into a training and test set with an 80/20 split of the data.


```{r question7b}
# Splitting data set by 80/20
set.seed(123)
split <- sample.split(spam$spam,SplitRatio = 0.80)
training_set <- subset(spam,split == TRUE)
test_set <- subset(spam,split == FALSE)
```


#### Question 7c

(8 points) Construct a classication tree to predict spam on the training data.

For R users, consider using the rpart library. Try using the default parameters of the software package. Understand how the decision tree is represented in R.

For Matlab users, consider using the Statistics Toolbox. This toolbox has several functions available for machine learning methods including classification trees.


```{r question7c}
# Fitting the decision tree to training set
dtree <- rpart(formula = spam ~.,data = training_set)

# Predicting the output for test set
pred_test <- predict(object = dtree,newdata = test_set,type = 'class')

# Creating confusion matrix
cm <- confusionMatrix(test_set[,15],pred_test)
cm
```


#### Question 7d

(6 points) Describe the tree that is constructed (print or plot the tree). How many terminal leaves does the tree have? What is the total number of nodes in the tree?


```{r question7d}
# Plotting Decision tree
plot(dtree)
text(dtree)

paste0("There are 8 terminal leaves")
paste0("There are 15 total nodes")
```


#### Question 7e

(6 points) Estimate the performance of the decision tree on the training set and the testing set. Report accuracy, error rate, and AUC using a threshold of 0.5.


```{r question7e}
# ROC Curve for training set
pred_training <- predict(object = dtree,newdata = training_set,type = 'class')
nb.pred <- prediction(as.numeric(pred_training), as.numeric(training_set$spam))
nb.roc.perf <- performance(nb.pred, "tpr", "fpr")
plot(nb.roc.perf,xlab="False positive rate",ylab="True positive rate")
pROC::auc(as.numeric(training_set$spam),as.numeric(pred_training),thresh = 0.5)

# ROC Curve for test set
nb.pred <- prediction(as.numeric(pred_test), as.numeric(test_set$spam))
nb.roc.perf <- performance(nb.pred, "tpr", "fpr")
plot(nb.roc.perf,xlab="False positive rate",ylab="True positive rate")
pROC::auc(as.numeric(test_set$spam),as.numeric(pred_test),thresh = 0.5)

# Confusion Matrix
cm <- confusionMatrix(test_set[,15],pred_test)
cm
paste0("The accuracy is 0.9194 and the error rate is 0.0806")
```


#### Question 7f

(8 points) Try pruning the tree, explore 2 other sized tree and report the classification performance in either case.


```{r question7f}
pruned <- prune(dtree,cp=dtree$cptable[which.min(dtree$cptable[,"xerror"]),"CP"])
prp(pruned)
```


#### Question 8

#### Classification of Music Popularity

For this problem, you will work to classify a song's popularity. Specifically, you will develop methods to predict whether a song will make the Top10 of Billboard's Hot 100 Chart. The data set consists of song from the Top10 of Billboard's Hot 100 Chart from 1990-2010 along with a sampling of other songs that did not make the list1.

The variables included in the data set include several description of the song and artist (including song title and id numbers), the year the song was released. Additionally, several variables describe the song attributes: time signature, loudness, tempo, key, energy pitch, and timbre (measured of different sections of the song). The last variable is binary indicated whether the song was in the Top10 or not.

You will use the variables of the song attributes (excluding the variables involving confidence in that attribute, e.g., key confidence) to predict whether the song will be popular or not.


#### Question 8a

Load in the music data. You should not use the artist or song title and IDs in the prediction along with the confidence variables.


```{r question8a}
# Loading the music data
music <- read.csv("data/music.csv")
```


#### Question 8b

Prepare the data for a 10-fold cross-validation. Ensure that each split of the data has a balanced distribution of class labels.


```{r question8b}
# Data preprocessing for 10 fold cross validation

# Setting the seed for reproduciablility
set.seed(294)

# NO of fold
n_fold <- 10

# Creating the folds variable in the data frame
music$folds <- createFolds(music$Top10, k = 10, list = FALSE)

# Checking the class label proportions
for(i in 1:n_fold) {
    fold <- music$Top10[music$folds == i]
    fold_length <- length(fold)
    class0_prop <- round(length(fold[fold == 0]) / fold_length, 2)
    class1_prop <- round(length(fold[fold == 1]) /  fold_length, 2)
    print(paste("Fold" , i , "length", fold_length, sep = " "))
    print(paste("Class Label(0)", round(class0_prop, 2), sep = " "))
    print(paste("Class Label(1)", round(class1_prop, 2), sep = " "))
}

# Creating the 10 data frames from the folds
music_10fold_list <- list()
for(i in 1:n_fold)  {
    music_10fold_list <- list(music_10fold_list, music[music$folds == i,])
}
```


#### Question 8c

(15 points) Use kNN to predict whether a song is a hit. Estimate the generalization performance over the 10-folds, calculate and report the accuracy, error, and AUC performance on the testing data. Show these results for three values of k = 1; 3; 5; 7; 9.


```{r question8c}
# Setting the k values for KNN
k_values <- c(1, 3, 5, 7, 9)



```


#### Question 8d

(12 points) Use decision trees to predict whether a song is a hit. Estimate the generalization performance over the 10-folds, calculate and report the accuracy, error, and AUC performance on the testing data. Show the results for two different sized decision trees (consider different amounts of pruning).


```{r question8d}

```


#### Question 8e

(15 points) Use a Naive Bayes classifier to predict whether a song is a hit. Calculate and report the accuracy, error, and AUC performance on the testing data.


```{r question8e}

```


#### Question 8f

(15 points) Use Random Forests to predict whether a song is a hit. Calculate and report the accuracy, error, and AUC performance on the testing data.


```{r question8f}

```


#### Question 8g

(20 points) Learn a support vector machine (SVM) with a RBF kernel to predict whether the song is a hit. Consider at least the following values for cost: 0.01, 0.1, 1, 10, 100. Calculate and report the accuracy, error, and AUC performance on the testing data for the best model found.


```{r question8g}

```


#### Question 8h

(5 points) Discuss whether the selection of the negative samples included in the data set may influence the results.


```{r question8h}

```


#### Question 8i

(5 points (bonus)) Re-run the analysis in (c) using a nested cross-validation to determine the best value of k and to determine the best parameters of the SVM.


```{r question8i}

```


**End of the assignment**