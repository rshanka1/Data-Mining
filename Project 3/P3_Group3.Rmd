---
title: "Project 3 Group 3"
author: "Nishanth Gandhidoss, Raghavendar Shankar"
date: "4 March 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r include=FALSE}
# installing the packages
# Function to Install packages
# checks the available.packages before
installNewPackage <- function(packageName) {
        if(packageName  %in% rownames(installed.packages()) == FALSE)
        {
                install.packages(packageName, repos = "http://cran.us.r-project.org", dependencies=TRUE)
        }
}

installNewPackage("ROCR")
installNewPackage("plyr")
installNewPackage("caret")
installNewPackage("class")
installNewPackage("MLmetrics")
installNewPackage("rpart")
installNewPackage("rpart.plot")
installNewPackage("rattle")
installNewPackage("randomForest")
installNewPackage("e1071")

library(ROCR)
library(plyr)
library(caret)
library(class)
library(MLmetrics)
library(rpart)
library(rpart.plot)
library(rattle)
library(randomForest)
library(e1071)
# library(httpuv)
# library(curlconverter)
# 
# # Comment it when your executing for second or the next time
# # for faster running 
# install_github("ggbiplot","vqv")
```

#### Question 1

(24 points) Write a function that will evaluate the predictions of a classifier. That is, given a vector of predicted values, Y~pred~, and actual class values, Y~true~, can report the following quantities:

* number of true positives (TP),
* number of false positives (FP),
* number of true negatives (TN),
* number of false negatives (FN),
* true positive rate (TPR),
* true negative rate (TNR),
* sensitivity (Sens.),
* specificity (Spec.),
* precision (Prec.),
* recall (Rec.),
* accuracy (Acc.), and
* error rate (Err)


```{r question1}
# Creting S4 object for the predcition evaluation
setClass("prediction", representation(predictions = "list", labels = "list", cutoffs = "list",
                                      fp = "list", tp = "list", tn = "list", fn = "list", 
                                      n.pos = "list", n.neg = "list", n.pos.pred = "list",
                                      n.neg.pred = "list", tpr = "numeric", fpr = "numeric", 
                                      tnr = "numeric", fnr = "numeric", sensitivity = "numeric",
                                      specificity = "numeric", precision = "numeric", 
                                      recall = "numeric", accuracy = "numeric", error_rate = "numeric"))

# Function to Evaluate the prediction
# Returns the necessary measures depeing 
# upon the passed argument
predictions <- function (predictions, labels, label.ordering = NULL) {
    # Check if data frame
    if (is.data.frame(predictions)) {
        names(predictions) <- c()
        predictions <- as.list(predictions)
    }
    # Check if it is matrix
    else if (is.matrix(predictions)) {
        predictions <- as.list(data.frame(predictions))
        names(predictions) <- c()
    }
    # Check if it is Vector
    else if (is.vector(predictions) && !is.list(predictions)) {
        predictions <- list(predictions)
    }
    # Stop if prediction is not of type list
    else if (!is.list(predictions)) {
        stop("Format of predictions is invalid.")
    }
    # Assign the labels
    if (is.data.frame(labels)) {
        names(labels) <- c()
        labels <- as.list(labels)
    }
    else if (is.matrix(labels)) {
        labels <- as.list(data.frame(labels))
        names(labels) <- c()
    }
    else if ((is.vector(labels) || is.ordered(labels) || is.factor(labels)) && 
        !is.list(labels)) {
        labels <- list(labels)
    }
    # Stop if labels is not of type list
    else if (!is.list(labels)) {
        stop("Format of labels is invalid.")
    }
    # Length check for single and multiple evaluations
    if (length(predictions) != length(labels)) 
        stop(paste("Number of cross-validation runs must be equal", 
            "for predictions and labels."))
    if (!all(sapply(predictions, length) == sapply(labels, length))) 
        stop(paste("Number of predictions in each run must be equal", 
            "to the number of labels for each run."))
    
    for (i in 1:length(predictions)) {
        finite.bool <- is.finite(predictions[[i]])
        predictions[[i]] <- predictions[[i]][finite.bool]
        labels[[i]] <- labels[[i]][finite.bool]
    }
    
    label.format = ""
    
    # Assign label fotmat for each type
    if (all(sapply(labels, is.factor)) && !any(sapply(labels, 
        is.ordered))) {
        label.format <- "factor"
    }
    else if (all(sapply(labels, is.ordered))) {
        label.format <- "ordered"
    }
    else if (all(sapply(labels, is.character)) || all(sapply(labels, 
        is.numeric)) || all(sapply(labels, is.logical))) {
        label.format <- "normal"
    }
    else {
        stop(paste("Inconsistent label data type across different", 
            "cross-validation runs."))
    }
    if (!all(sapply(labels, levels) == levels(labels[[1]]))) {
        stop(paste("Inconsistent factor levels across different", 
            "cross-validation runs."))
    }
    levels <- c()
    if (label.format == "ordered") {
        if (!is.null(label.ordering)) {
            stop(paste("'labels' is already ordered. No additional", 
                "'label.ordering' must be supplied."))
        }
        else {
            levels <- levels(labels[[1]])
        }
    }
    else {
        if (is.null(label.ordering)) {
            if (label.format == "factor") 
                levels <- sort(levels(labels[[1]]))
            else levels <- sort(unique(unlist(labels)))
        }
        else {
            if (!setequal(unique(unlist(labels)), label.ordering)) {
                stop("Label ordering does not match class labels.")
            }
            levels <- label.ordering
        }
        for (i in 1:length(labels)) {
            if (is.factor(labels)) 
                labels[[i]] <- ordered(as.character(labels[[i]]), 
                  levels = levels)
            else labels[[i]] <- ordered(labels[[i]], levels = levels)
        }
    }
    if (length(levels) != 2) {
        message <- paste("Number of classes is not equal to 2.\n", 
            "ROCR currently supports only evaluation of ", "binary classification tasks.", 
            sep = "")
        stop(message)
    }
    if (!is.numeric(unlist(predictions))) {
        stop("Currently, only continuous predictions are supported by ROCR.")
    }
    
    # Intailizing empty list
    cutoffs <- list()
    fp <- list()
    tp <- list()
    fn <- list()
    tn <- list()
    n.pos <- list()
    n.neg <- list()
    n.pos.pred <- list()
    n.neg.pred <- list()
    
    # Loop to perform the caluclation for evaluation metrics
    for (i in 1:length(predictions)) {
        n.pos <- c(n.pos, sum(labels[[i]] == levels[2]))
        n.neg <- c(n.neg, sum(labels[[i]] == levels[1]))
        ans <- .compute.unnormalized.roc.curve(predictions[[i]], 
            labels[[i]])
        cutoffs <- c(cutoffs, list(ans$cutoffs))
        fp <- c(fp, list(ans$fp))
        tp <- c(tp, list(ans$tp))
        fn <- c(fn, list(n.pos[[i]] - tp[[i]]))
        tn <- c(tn, list(n.neg[[i]] - fp[[i]]))
        n.pos.pred <- c(n.pos.pred, list(tp[[i]] + fp[[i]]))
        n.neg.pred <- c(n.neg.pred, list(tn[[i]] + fn[[i]]))
    }
    tpr <- tp[[1]] / max(tp[[1]])
    fpr <- fp[[1]] / max(fp[[1]])
    tnr <- tn[[1]] / max(tn[[1]])
    fnr <- fn[[1]] / max(fn[[1]])
    sensitivity <- tp[[1]] / (tp[[1]] + fn[[1]]) 
    specificity <- tn[[1]] / (fp[[1]] + tn[[1]])
    precision <- tp[[1]] / (tp[[1]] + fp[[1]])
    accuracy <- (tp[[1]] + tn[[1]]) / (tp[[1]] + fp[[1]] + fn[[1]] + tn[[1]])
    error_rate <- 1 - accuracy
    
    return(new("prediction", predictions = predictions, labels = labels, 
        cutoffs = cutoffs, fp = fp, tp = tp, fn = fn, tn = tn, 
        n.pos = n.pos, n.neg = n.neg, n.pos.pred = n.pos.pred, 
        n.neg.pred = n.neg.pred, tpr = tpr, fpr = fpr, tnr = tnr, fnr = fnr,
        sensitivity = sensitivity, specificity = specificity, precision = precision,
        recall = sensitivity, accuracy = accuracy, error_rate = error_rate))
}
```


#### Question 2

(20 points) For the following data set, you will compute the true positive rate, false positive rate, and accuracy (with the function you wrote for Prob. 1) using every predicted output as a possible threshold.


Sample  |   Y~true~   |   Y~Pred~
--------|-------------|------------
    1   |      1      |   0.98    
    2   |      0      |   0.92
    3   |      1      |   0.85
    4   |      1      |   0.77
    5   |      0      |   0.71
    6   |      0      |   0.64
    7   |      1      |   0.50
    8   |      0      |   0.39
    9   |      1      |   0.34
    10  |      0      |   0.31


```{r question2}
# Assigning the given data to varibles
y_true <- c(1, 0, 1, 1, 0 ,0, 1, 0, 1, 0)
y_pred <- c(0.98, 0.92, 0.85, 0.77, 0.71, 0.64, 0.50, 0.39, 0.34, 0.31)

# Calling the prediction function
pred <- predictions(y_pred, y_true)

# Creating dataframe for the required result
result_q2 <- data.frame(true.positve.rate = pred@tpr[-1], false.positive.rate = pred@fpr[-1],
                        accuracy = pred@accuracy[-1])

# Printing the reults
result_q2
```


#### Question 3

(10 points) Use the results from Prob. 2 to plot the ROC curve for the data. Note, plot this curve using the standard plotting tools rather than any special library/package available in R or Matlab.


```{r question3}
# Plotting the ROC curve for the data
plot(result_q2$false.positive.rate, result_q2$true.positve.rate, type = "l", col = "red")
abline(a=0, b=1)
```

#### Question 4

(12 points) Data Mining Book, 8.14.

Suppose that we want to select between two prediction models, M~1~ and M~2~. We have performed 10 rounds of 10-fold cross-validation on each model, where the same data partitioning in round i is used for both M~1~ and M~2~. The error rates obtained for M~1~ are 30.5, 32.2, 20.7, 20.6, 31.0, 41.0, 27.7, 26.0, 21.5, 26.0. The error rates for M~2~ are 22.4, 14.5, 22.4, 19.6, 20.7, 20.4, 22.1, 19.4, 16.2, 35.0. Comment on whether one model is significantly better than the other considering a significance level of 1%.


```{r question4}

```


#### Question 5

Suppose you are building a decision tree on a data set with three classes A;B;C. At the current position in the tree you have the following samples available:

$$
N = \left(\begin{array}{cc} 
A & 100\\
B & 50\\
C & 60\\
\end{array}\right)
$$ 


You are examining two ways to split the data. The first splits the data as,

$$
N~1,1~ = \left(\begin{array}{cc} 
A & 62\\
B & 8\\
C & 0\\
\end{array}\right)
$$
$$
N~1,2~ = \left(\begin{array}{cc} 
A & 38\\
B & 42\\
C & 60\\
\end{array}\right)
$$

The second splits the data as,

$$
N~2,1~ = \left(\begin{array}{cc} 
A & 65\\
B & 20\\
C & 0\\
\end{array}\right)
$$
$$
N~2,2~ = \left(\begin{array}{cc} 
A & 21\\
B & 19\\
C & 20\\
\end{array}\right)
$$
$$
N~2,3~ = \left(\begin{array}{cc} 
A & 14\\
B & 11\\
C & 40\\
\end{array}\right)
$$ 


#### Question 5a

(a) (18 points) Compute the gain in GINI index for the two splits. Show the form of the calculations, not just the final numbers.


```{r question5a}

```


#### Question 5b

(4 points) Which node would be preferred to include next in the decision tree?


```{r question5b}

```


#### Question 5c

(18 points) Compute the information gain (based on entropy) for the two splits. Show the form of the calculations, not just the final numbers.


```{r question5c}

```


#### Question 5d

(4 points) Which node would be preferred to include next in the decision tree?


```{r question5d}

```


#### Question 6

#### Classification of Age based on Social Media Usage

For this problem, you want to classify the age of an individual ("High School" or "Adult") basic on their social media app usage. The data was collected via a survey, with respondents consisting of the Women in Computing Sciences Summer Youth Program participants and female faculty at Michigan Tech. The data is available at syp-s16-data.csv. There are 60 responses with 26 Adults and 34 HS respondents.


#### Question 6a

(12 points) Create a small multiples plot showing the number of respondents who use or do not use each app grouped by age. Consider making grouped bar plots.


```{r question6a}

```


#### Question 6b

(4 points) From the figures in (a), which of the apps would you expect to find at the root of a deicision tree? that is, which app (variable) would be best to separate the two classes of responses?


```{r question6b}

```


#### Question 6c

(10 points) Construct a decision tree using this data set. Does the variable at the root of the tree match the intuition from part (b)?


```{r question6c}

```


#### Question 7

#### Classification of Spam: Trees

For this problem, you will work to classify e-mail messages as spam or not. The data set to be used is given with the project (note, this is not the same spam data set that is available from UCI ML repository).


#### Question 7a

Load in the spam data. You should not include the following columns in the classification task: isuid, id, domain, spampct, category, and cappct.


```{r question7a}

```


#### Question 7b

(4 points) Split the data into a training and test set with an 80/20 split of the data.


```{r question7b}

```


#### Question 7c

(8 points) Construct a classification tree to predict spam on the training data.

For R users, consider using the rpart library. Try using the default parameters of the software package. Understand how the decision tree is represented in R.

For Matlab users, consider using the Statistics Toolbox. This toolbox has several functions available for machine learning methods including classification trees.


```{r question7c}

```


#### Question 7d

(6 points) Describe the tree that is constructed (print or plot the tree). How many terminal leaves does the tree have? What is the total number of nodes in the tree?


```{r question7d}

```


#### Question 7e

(6 points) Estimate the performance of the decision tree on the training set and the testing set. Report accuracy, error rate, and AUC using a threshold of 0.5.


```{r question7e}

```


#### Question 7f

(8 points) Try pruning the tree, explore 2 other sized tree and report the classification performance in either case.


```{r question7f}

```


#### Question 8

#### Classification of Music Popularity

For this problem, you will work to classify a song's popularity. Specifically, you will develop methods to predict whether a song will make the Top10 of Billboard's Hot 100 Chart. The data set consists of song from the Top10 of Billboard's Hot 100 Chart from 1990-2010 along with a sampling of other songs that did not make the list1.

The variables included in the data set include several description of the song and artist (including song title and id numbers), the year the song was released. Additionally, several variables describe the song attributes: time signature, loudness, tempo, key, energy pitch, and timbre (measured of different sections of the song). The last variable is binary indicated whether the song was in the Top10 or not.

You will use the variables of the song attributes (excluding the variables involving confidence in that attribute, e.g., key confidence) to predict whether the song will be popular or not.


#### Question 8a

Load in the music data. You should not use the artist or song title and IDs in the prediction along with the confidence variables.


```{r question8a}
# Loading the music data
music_real <- read.csv("data/music.csv")

# Removing IDs, artist, song title variables out of the data 
music <-  music_real[, -c(2, 3, 4, 5, 6 , 9, 11)]

# Checking for NA values in the data
sapply(music, function(x) sum(is.na(x)))
```

It looks like the data has no NA values that to be concerned about.

#### Question 8b

Prepare the data for a 10-fold cross-validation. Ensure that each split of the data has a balanced distribution of class labels.


```{r question8b}
# Data preprocessing for 10 fold cross validation

# Setting the seed for reproduciablility
set.seed(294)

# NO of fold
k_fold <- 10

# Creating the folds variable in the data frame
# music$folds <- createFolds(music$Top10, k = n_fold, list = FALSE)
cv_index_list <- createFolds(music$Top10, k = k_fold, list = TRUE, returnTrain = FALSE)

music_10fold_list <- list()

# Checking the class label proportions
for(i in 1:k_fold) {
    fold <- music$Top10[cv_index_list[[i]]]
    fold_length <- length(fold)
    class0_prop <- round(length(fold[fold == 0]) / fold_length, 2)
    class1_prop <- round(length(fold[fold == 1]) /  fold_length, 2)
    print(paste("Fold" , i , "length", fold_length, sep = " "))
    print(paste("Class Label(0)", round(class0_prop, 2), sep = " "))
    print(paste("Class Label(1)", round(class1_prop, 2), sep = " "))
    # Creating/appending the 10 data frames from the folds
    music_10fold_list <- list(music_10fold_list, music[music$folds == i,])
}
```


#### Question 8c

(15 points) Use kNN to predict whether a song is a hit. Estimate the generalization performance over the 10-folds, calculate and report the accuracy, error, and AUC performance on the testing data. Show these results for three values of k = 1; 3; 5; 7; 9.


```{r question8c}
# Setting the k values for KNN
k_values <- c(1, 3, 5, 7, 9)

# Running the KNN for the mentioned K values
# with 10 - fold cross validation
# Accuracy, error, AUC are rounded off to two decimal points
for(k in k_values) {
    cat("\n")
    print(paste("***Knn with K =", k, "***", sep = " "))
    cat("\n")
    total_acc <- 0
    total_error_rate <- 0
    total_AUC <- 0
    for(fold in 1:k_fold) {
        train_set <- music[-cv_index_list[[fold]], ] 
        test_set <- music[cv_index_list[[fold]], ] 
        knn_predicted_values <- knn(train = train_set, test = test_set, cl = train_set$Top10, k = 1)
        print(paste("Test set fold #", fold, sep = " "))
        cat("\n")
        accuracy <- round(Accuracy(knn_predicted_values, test_set$Top10), 2)
        error_rate <- 1 - accuracy
        AUC <- round(AUC(knn_predicted_values, test_set$Top10), 2)
        print(paste("Accuracy is", accuracy, sep = " "))
        print(paste("Error is", error_rate, sep =" "))
        print(paste("Area under the curve(AUC) is", AUC, sep = " "))
        cat("\n")
        total_acc <- total_acc + accuracy
        total_error_rate <- total_error_rate + error_rate
        total_AUC <- total_AUC + AUC
    }
    cat("==========================================================\n")
    print(paste("Overall accuracy is", round(total_acc / k_fold, 2), sep = " "))
    print(paste("Overall error is", round(total_error_rate / k_fold, 2), sep =" "))
    print(paste("Overall area under the curve(AUC) is", round(total_AUC / k_fold, 2), sep = " "))
    cat("\n==========================================================\n")
}
```


#### Question 8d

(12 points) Use decision trees to predict whether a song is a hit. Estimate the generalization performance over the 10-folds, calculate and report the accuracy, error, and AUC performance on the testing data. Show the results for two different sized decision trees (consider different amounts of pruning).


```{r question8d}
# Intailize variables
total_acc <- 0
total_error_rate <- 0
total_AUC <- 0

# Running the Decision Tree with 10 - fold cross validation
# Accuracy, error, AUC are rounded off to two decimal points
for(fold in 1:k_fold) {
    train_set <- music[-cv_index_list[[1]], ] 
    test_set <- music[cv_index_list[[1]], ] 
    tree_fit <- rpart(Top10 ~ ., method="class", data = train_set)
    
    fancyRpartPlot(tree_fit, main = paste("Before Pruning - Fold #", fold, sep = " "))
    
    # Find cp with minimum Cross validate error from rpart results
    min_error_cp <- tree_fit$cptable[which.min(tree_fit$cptable[,"xerror"]), "CP"]
    
    # pruning the tree with minimum cross validate error
    ptree_fit <- prune(tree_fit, cp = min_error_cp)
    fancyRpartPlot(ptree_fit, main = paste("After Pruning - Fold #", fold, sep = " "))
    
    # Calculating Accuracy. Error, AUC
    test_prediction <- predict(tree_fit, test_set, type = 'class')
    print(paste("Test set fold #", fold, sep = " "))
    cat("\n")
    accuracy <- round(Accuracy(test_prediction, test_set$Top10), 2)
    error_rate <- 1 - accuracy
    AUC <- round(AUC(test_prediction, test_set$Top10), 2)
    print(paste("Accuracy is", accuracy, sep = " "))
    print(paste("Error is", error_rate, sep =" "))
    print(paste("Area under the curve(AUC) is", AUC, sep = " "))
    total_acc <- total_acc + accuracy
    total_error_rate <- total_error_rate + error_rate
    total_AUC <- total_AUC + AUC
    cat("\n")
}

cat("==========================================================\n")
print(paste("Overall accuracy is", round(total_acc / k_fold, 2), sep = " "))
print(paste("Overall error is", round(total_error_rate / k_fold, 2), sep =" "))
print(paste("Overall area under the curve(AUC) is", round(total_AUC / k_fold, 2), sep = " "))
cat("\n==========================================================\n")
```


#### Question 8e

(15 points) Use a Naive Bayes classifier to predict whether a song is a hit. Calculate and report the accuracy, error, and AUC performance on the testing data.


```{r question8e}
# Intailize variables
total_acc <- 0
total_error_rate <- 0
total_AUC <- 0

# Running the Decision Tree with 10 - fold cross validation
# Accuracy, error, AUC are rounded off to two decimal points
for(fold in 1:k_fold) {
    train_set <- music[-cv_index_list[[fold]], ] 
    test_set <- music[cv_index_list[[fold]], ] 
    
    # Fitting the Naive bayes model
    nb_fit <- naiveBayes(as.factor(Top10) ~ ., data = train_set)
    
    # Calculating Accuracy. Error, AUC
    test_prediction <- predict(nb_fit, test_set, type = "class")
    
    print(paste("Test set fold #", fold, sep = " "))
    cat("\n")
    accuracy <- round(Accuracy(test_prediction, test_set$Top10), 2)
    error_rate <- 1 - accuracy
    AUC <- round(AUC(test_prediction, test_set$Top10), 2)
    print(paste("Accuracy is", accuracy, sep = " "))
    print(paste("Error is", error_rate, sep =" "))
    print(paste("Area under the curve(AUC) is", AUC, sep = " "))
    total_acc <- total_acc + accuracy
    total_error_rate <- total_error_rate + error_rate
    total_AUC <- total_AUC + AUC
    cat("\n")
}

cat("==========================================================\n")
print(paste("Overall accuracy is", round(total_acc / k_fold, 2), sep = " "))
print(paste("Overall error is", round(total_error_rate / k_fold, 2), sep =" "))
print(paste("Overall area under the curve(AUC) is", round(total_AUC / k_fold, 2), sep = " "))
cat("\n==========================================================\n")
```


#### Question 8f

(15 points) Use Random Forests to predict whether a song is a hit. Calculate and report the accuracy, error, and AUC performance on the testing data.


```{r question8f}
# Intailize variables
total_acc <- 0
total_error_rate <- 0
total_AUC <- 0

# Running the Random Forest with 10 - fold cross validation
# Accuracy, error, AUC are rounded off to two decimal points
for(fold in 1:k_fold) {
    train_set <- music[-cv_index_list[[fold]], ] 
    test_set <- music[cv_index_list[[fold]], ] 
    
    # Fitting the model
    rf_fit <- randomForest(as.factor(Top10) ~ ., data = train_set, importance = TRUE, ntree = 100)
    
    # Calculating Accuracy. Error, AUC
    test_prediction <- predict(rf_fit, test_set, type = 'class')
    print(paste("Test set fold #", fold, sep = " "))
    cat("\n")
    accuracy <- round(Accuracy(test_prediction, test_set$Top10), 2)
    error_rate <- 1 - accuracy
    AUC <- round(AUC(test_prediction, test_set$Top10), 2)
    print(paste("Accuracy is", accuracy, sep = " "))
    print(paste("Error is", error_rate, sep =" "))
    print(paste("Area under the curve(AUC) is", AUC, sep = " "))
    total_acc <- total_acc + accuracy
    total_error_rate <- total_error_rate + error_rate
    total_AUC <- total_AUC + AUC
    cat("\n")
}

cat("==========================================================\n")
print(paste("Overall accuracy is", round(total_acc / k_fold, 2), sep = " "))
print(paste("Overall error is", round(total_error_rate / k_fold, 2), sep =" "))
print(paste("Overall area under the curve(AUC) is", round(total_AUC / k_fold, 2), sep = " "))
cat("\n==========================================================\n")
```


#### Question 8g

(20 points) Learn a support vector machine (SVM) with a RBF kernel to predict whether the song is a hit. Consider at least the following values for cost: 0.01, 0.1, 1, 10, 100. Calculate and report the accuracy, error, and AUC performance on the testing data for the best model found.


```{r question8g}
# Intializing the cost variables
cost_values <- c(0.01, 0.1, 1, 10, 10, 100)
total_acc_list <- c()
total_error_rate_list <- c()
total_AUC_list <- c()

# Running the Support Vector machine for cost values 
# 0.01, 0.1, 1, 10, 10, 100 with 10 - fold cross validation
# Accuracy, error, AUC are rounded off to two decimal points
for(cost in cost_values) {
    
    # Intailize variables
    total_acc <- 0
    total_error_rate <- 0
    total_AUC <- 0
    
    print(paste("*** For the cost value", cost, "***", sep = " "))
    
    for(fold in 1:k_fold) {
        train_set <- music[-cv_index_list[[fold]], ] 
        test_set <- music[cv_index_list[[fold]], ] 
        
        # Fitting the model
        svm_fit <- svm(as.factor(Top10) ~ ., data = train_set, kernel = "radial", cost = cost)
        
        # Calculating Accuracy. Error, AUC
        test_prediction <- predict(rf_fit, test_set, type = 'class')
        print(paste("Test set fold #", fold, sep = " "))
        cat("\n")
        accuracy <- round(Accuracy(test_prediction, test_set$Top10), 2)
        error_rate <- 1 - accuracy
        AUC <- round(AUC(test_prediction, test_set$Top10), 2)
        print(paste("Accuracy is", accuracy, sep = " "))
        print(paste("Error is", error_rate, sep =" "))
        print(paste("Area under the curve(AUC) is", AUC, sep = " "))
        total_acc <- total_acc + accuracy
        total_error_rate <- total_error_rate + error_rate
        total_AUC <- total_AUC + AUC
        cat("\n")
    }
    total_acc_list <- c(total_acc_list, total_acc / k_fold)
    total_error_rate_list <- c(total_error_rate_list, total_error_rate / k_fold)
    total_AUC_list <- c(total_AUC_list, total_AUC / k_fold)
}

max_acc_index <- which.max(total_acc_list)
best_cost <- cost_values[max_acc_index]
best_acc <- total_acc_list[max_acc_index]
best_error_rate <- total_error_rate_list[max_acc_index]
best_AUC <- total_AUC_list[max_acc_index]

cat("==========================================================\n")
cat(paste("Best model is one with cost as", best_cost, "\n", sep = " "))
print(paste("Best model accuracy is", round(best_acc, 2), sep = " "))
print(paste("Best model error is", round(best_error_rate, 2), sep =" "))
print(paste("Best model area under the curve(AUC) is", round(best_AUC, 2), sep = " "))
cat("\n==========================================================\n")
```


#### Question 8h

(5 points) Discuss whether the selection of the negative samples included in the data set may influence the results.


```{r question8h}

```


#### Question 8i

(5 points (bonus)) Re-run the analysis in (c) using a nested cross-validation to determine the best value of k and to determine the best parameters of the SVM.


```{r question8i}

```


**End of the assignment**