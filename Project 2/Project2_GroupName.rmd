---
title: "Group Name"
author: "Nishanth Gandhidoss, Raghavendar Shankar, Mitul Shah"
date: "2 February 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r include=FALSE}
# installing the packages
# Function to Install packages
# checks the available.packages before
installNewPackage <- function(packageName) {
        if(packageName  %in% rownames(installed.packages()) == FALSE)
        {
                install.packages(packageName, repos = "http://cran.us.r-project.org", dependencies=TRUE)
        }
}

```


## Group Name

* Nishanth Gandhidoss  
* Raghavendar Shankar
* Mitul Shah


#### Question 1

(20 points) Write your own general-purpose functions to perform min-max normalization and z-score normalization (using standard deviation or mean absolute deviation). Do not just use functions available in R, Python or Matlab. Think about how to ensure it generalizes. For example, in R will it work on vectors, matrices and data frames. In Matlab, will it work on vectors and matrices. For min-max normalization, typically you want to scale to [0; 1] (defaults), but you should be able to pass in other limits.

The function minmaxNorm should take four arguments

* **trData**- the training data (use to establish the data properties for normalizaiton)
* **teData**- the testing data (if supplied) to also be normalized according the the same data properties
* **minV**- minimum value of new range
* **maxV**- maximum value of new range

The function zscoreNorm should take three arguments

* **trData**- the training data (use to establish the data properties for normalizaiton)
* **teData**- the testing data (if supplied) to also be normalized according the the same data properties
* **madFlag**- boolean flag if positive use mean absolute deviation instead of standard deviation


```{r This chunk is for testing}
dat <- read.csv("TaskRatings.csv")

longmat <- rbind(as.matrix(dat[,3:10]),
                  as.matrix(dat[,11:18]),
                  as.matrix(dat[,19:26]),deparse.level=2)

longdat <- data.frame(task=dat$Task,
                      rater= as.factor(rep(1:3,each=nrow(dat))),
                       longmat)
longdat <- longdat[, c(-1, -2)]
```



```{r question1(minmax)}
# Check whether the two data is of same class
isDataSameClass <- function(trData, teData) {
    if((class(trData) != class(teData)) & !is.null(teData)) {
        stop("Train and test data are of different class...")
    }
}

# Check whether Max greater than Min
isMaxVGreater <-  function(minV, maxV) {
    if(minV > maxV)
        stop("Maximum value argument should be larger than minimum value...")
}

# Create custome column names like V1, V2, etc
prepColNames <- function(data) {
    colname <- c()
    for(i in 1:ncol(data)) {
        colname <- c(colname, paste0("V", i))
    }
    colname
}

# Normalize train and test data in form vectors/column
getNormalized <- function(trData, teData, minV, maxV, madFlag, method) {
    
    if(!is.null(teData)) {
        if(!(is.numeric(trData)) | !is.numeric(teData)) {
            stop("Data supplied are not in numeric format...")
        }
        if(anyNA(trData) | anyNA(teData)) {
            stop("Your data contains NA...")
        }
    } else {
        if(!is.numeric(trData)) {
            stop("Data supplied are not in numeric format...")
        }
        if(anyNA(trData)) {
            stop("Your data contains NA...")
        }
    }

    trResult <- NULL
    teResult <- NULL
    result <- NULL
    
    if(method == "minmax") {
        trMin <- min(trData)
        trMax <- max(trData)
        for(i in 1:length(trData)) {
            trResult[i] <- (maxV - minV) / (trMax -  trMin) * (trData[i] - trMax) + maxV
        }
        if(!is.null(teData)) {
            teMin <- min(teData)
            teMax <- max(teData)
            for(i in 1:length(teData)) {
                teResult[[i]] <- (maxV - minV) / (teMax -  teMin) * (teData[i] - teMax) + maxV
            }
            result <- append(list(trResult), list(teResult))
        } else {
            result <- list(trResult)
        }
    } else if(method == "zscore"){
        if(!madFlag) {
            trMean <- mean(trData)
            trSd <- sd(trData)
            for(i in 1:length(trData)) {
                trResult[i] <- (trData[i] - trMean) / trSd 
            }
            if(!is.null(teData)) {
                teMean <- mean(teData)
                teSd <- sd(teData)
                for(i in 1:length(teData)) {
                    teResult[[i]] <- (teData[i] - teMean) / teSd 
                }
                result <- append(list(trResult), list(teResult))
            } else {
                result <- list(trResult)
            }
        } else {
            
        }
    }
    result
}

# Normalize train and test dataframe
getNormalizedDF <- function(trData, teData, minV, maxV, madFlag, method) {
    trResultList <- list()
    teResultList <- list()
    for(i in 1:ncol(trData)) {
        if(!is.null(teData)) {
            scaled <- getNormalized(trData[, i], teData[, i], minV, maxV, madFlag, method)
            teResultList[[i]] <- scaled[2]
        } else {
            scaled <- getNormalized(trData[, i], teData, minV, maxV, madFlag, method)
        }
        trResultList[[i]] <- scaled[1]
    }
    trResult <- data.frame(trResultList)
    colnames(trResult) <- prepColNames(trResult)
    if(is.null(teData)) {
        result <- trResult
    } else {
        teResult <- data.frame(teResultList)
        colnames(teResult) <- prepColNames(teResult)
        result <- list(trResult, teResult)
    }
    result
}

# Function to compute Min Max Normalization
minmaxNorm <- function(trData = NULL, teData = NULL, minV = NULL, maxV = NULL) {
    if(!is.null(trData)) {
        isDataSameClass(trData, teData)
        isMaxVGreater(minV, maxV)
        method <- "minmax"
        if(is.vector(trData)) {
            result <- getNormalized(trData, teData, minV, maxV, NULL, method)
        } else if(is.data.frame(trData)) {
            result <- getNormalizedDF(trData, teData, minV, maxV, NULL, method)
        } else if(is.matrix(trData)) {
            if(is.null(teData)) {
                result <- getNormalizedDF(as.data.frame(trData), teData, minV, maxV, NULL, method)
            } else {
                result <- getNormalizedDF(as.data.frame(trData), 
                                                as.data.frame(teData), minV, maxV, NULL, method)
            }
        }
    } else {
        stop("Train data is required and cannot be NULL...")
    }
    result
}


# minmaxNorm(c(200, 300, 400, 600,1000), NULL, 0, 1)
# minmaxNorm(c(200, 300, 400, 600,1000), c(43, 54, 32), 3, 5)
# minmaxNorm(longdat[1:250,], NULL, 0, 1)
# minmaxNorm(longdat[1:250,], longdat[251:321,], 2, 3)
# minmaxNorm(longmat[1:250,], NULL, 0, 1)
# minmaxNorm(longmat[1:250,], longmat[251:321,], 2, 3)
```


```{r question1(zscore)}
# Function to compute zscore Normalization
zscoreNorm <- function(trData = NULL, teData = NULL, madFlag = FALSE) {
    if(!is.null(trData)) {
        isDataSameClass(trData, teData)
        method <- "zscore"
        if(is.vector(trData)) {
            result <- getNormalized(trData, teData, NULL, NULL, madFlag, method)
        } else if(is.data.frame(trData)) {
            result <- getNormalizedDF(trData, teData, NULL, NULL, madFlag, method)
        } else if(is.matrix(trData)) {
            if(is.null(teData)) {
                result <- getNormalizedDF(as.data.frame(trData), teData, NULL, NULL, madFlag, method)
            } else {
                result <- getNormalizedDF(as.data.frame(trData), 
                                                as.data.frame(teData), NULL, NULL, madFlag, method)
            }
        }
    } else {
        stop("Train data is required and cannot be NULL...")
    }
    result
}

# zscoreNorm(c(200, 300, 400, 600,1000), NULL, FALSE)
# zscoreNorm(c(200, 300, 400, 600,1000), c(43, 54, 32), FALSE)
# zscoreNorm(longdat[1:250,], NULL, FALSE)
# zscoreNorm(longdat[1:250,], longdat[251:321,], FALSE)
# zscoreNorm(longmat[1:250,], NULL, FALSE)
# zscoreNorm(longmat[1:250,], longmat[251:321,], FALSE)
```


#### Question 2

(8 points) Data Mining Book: 3.6(a-c)

Report the normalized values in a table.


```{r question2}

```



#### Question 3

Load the Iris data set available from the UCI Machine Learning data repository, http://archive.ics.uci.edu/ml/.

Using the data for petal length, answer the following questions:

#### Question 3 a

(4 points) Use your min-max normalization function with a range [????1:0; 1:0], to what values would f1:95; 3:1; 5:68 and 6:2g transform?


```{r question3a}

```


#### Question 3 b

(4 points) Use your z-score normalization function to determine what values f1:95, 3:1, 5:68, and 6:2g would transform to?


```{r question3b}

```


#### Question 3 c

(2 points) Comment on which method is preferred for this data, and why?


```{r question3c}

```


#### Question 4

Consider the following data set of with 5 samples and 3 variables:

Table here

You have a new data point x = (1:25; 1:78; 3:01).

#### Question 4 a

(5 points) Calculate and present the distance between the new data point and each of the points in the data set using Manhattan distance, Euclidean distance, Minkowski distance ( = 3), supremum distance, and cosine similarity.


```{r question4a}
A <- c(1.4,1.8,1.3,0.9,1.5)
B <- c(1.3,1.4,1.2,3.5,2.1)
C <- c(2.9,3.2,2.9,3.1,3.3)

dat <- cbind.data.frame(A,B,C)

rownames(dat) <- c("x1","x2","x3","x4","x5")

x <- rbind.data.frame(c(1.25,1.78,3.01))
g <- rep(0,5)
```
```{r Manhattan Distance}
Man_fun <- function(m,v){
        for(i in 1:nrow(m)){
                p <- rep(0,3)
                for(j in 1:ncol(v)){
                        
                        p[j] <- abs(m[i,j] - v[1,j])
                        
                }
                g[i] <- sum(p)
        }
        return(g)
}

Man_fun(dat,x)
```
```{r Euclidean Distance}
Euc_fun <- function(m,v){
        for(i in 1:nrow(m)){
                p <- rep(0,3)
                for(j in 1:ncol(v)){
                       
                        p[j] <- (m[i,j] - v[1,j])^2
                        
                }
                g[i] <- sqrt(sum(p))
        }
        return(g)
}

Euc_fun(dat,x)
```
```{r Minkowski Distance}
Mink_fun <- function(m,v,L){
        for(i in 1:nrow(m)){
                p <- rep(0,3)
                for(j in 1:ncol(v)){
                        
                        p[j] <- (abs(m[i,j] - v[1,j]))^L
                        
                }
                g[i] <- (sum(p))^(1/L)
        }
        return(g)
}
Mink_fun(dat,x,L = 3)
```
```{r Supremum Distance}
Sup_fun <- function(m,v){
        for(i in 1:nrow(m)){
                p <- rep(0,3)
                for(j in 1:ncol(v)){
                        
                        p[j] <- abs(m[i,j] - v[1,j])
                        
                }
                g[i] <- max(p)
        }
        return(g)
}

Sup_fun(dat,x)
```
```{r Cosine Similarity}
#crossprod(x, y)/sqrt(crossprod(x) * crossprod(y))
```


#### Question 4 b

(5 points) Normalize the data using min-max normalization to be between 0 and 1. What is the Euclidean distance between the new data point and x1; : : : ; x5.


```{r question4b}

```


#### Question 5

**K-means Clustering**

Perform k-means clustering manually with k=2 on the example data given below of n = 8 samples over p = 2 features.

Table here


#### Question 5 a

(2 points) Plot the sample data.


```{r question5a}

```


#### Question 5 b

(4 points) Assign samples to be the initial groupings given in the table. Compute and report the centroid for each cluster.

```{r question5b}

```


#### Question 5 c

(4 points) Assign each sample to the centroid to which it is closest (Euclidean distance). Report the cluster labels for each observation.

```{r question5c}

```


#### Question 5 d

(20 points) Repeat (b) and (c) until the clusters remain stable.


```{r question5d}

```


#### Question 5 e

(2 points) Plot the sample data colored by cluster labeling and adding centroid points.


```{r question5e}

```


#### Question 6

**Hierarchical Clustering**

Suppose you have 5 samples, for which the dissimilarity matrix is shown below:

Matrix here

That is, the distance between the first and second sample is 0.3; the distance between the first and fourth sample is 0.7.


#### Question 6 a

(12 points) Trace running through hierarchical clustering manually with complete link- age and sketch the dendrogram. Estimate the heights in the dendrogram from the dissimilarity distances.


```{r question6a}

```


#### Question 6 b

(12 points) Repeat (a), with single linkage clustering


```{r question6b}

```


#### Question 6 c

(6 points) Use the dendrogram from (a) and (b), cut the dendrograms to form three clusters. Which samples are in each cluster?


```{r question6c}

```


**End of assignemnt**