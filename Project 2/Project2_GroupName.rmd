---
title: "Group Name"
author: "Nishanth Gandhidoss, Raghavendar Shankar, Mitul Shah"
date: "2 February 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r include=FALSE}
# installing the packages
# Function to Install packages
# checks the available.packages before
installNewPackage <- function(packageName) {
        if(packageName  %in% rownames(installed.packages()) == FALSE)
        {
                install.packages(packageName, repos = "http://cran.us.r-project.org", dependencies=TRUE)
        }
}

installNewPackage("lsr")

library(lsr)
```


## Group Name

* Nishanth Gandhidoss  
* Raghavendar Shankar
* Mitul Shah


#### Question 1

(20 points) Write your own general-purpose functions to perform min-max normalization and z-score normalization (using standard deviation or mean absolute deviation). Do not just use functions available in R, Python or Matlab. Think about how to ensure it generalizes. For example, in R will it work on vectors, matrices and data frames. In Matlab, will it work on vectors and matrices. For min-max normalization, typically you want to scale to [0; 1] (defaults), but you should be able to pass in other limits.

The function minmaxNorm should take four arguments

* **trData**- the training data (use to establish the data properties for normalizaiton)
* **teData**- the testing data (if supplied) to also be normalized according the the same data properties
* **minV**- minimum value of new range
* **maxV**- maximum value of new range

The function zscoreNorm should take three arguments

* **trData**- the training data (use to establish the data properties for normalizaiton)
* **teData**- the testing data (if supplied) to also be normalized according the the same data properties
* **madFlag**- boolean flag if positive use mean absolute deviation instead of standard deviation


```{r This chunk is for testing}
dat <- read.csv("TaskRatings.csv")

longmat <- rbind(as.matrix(dat[,3:10]),
                  as.matrix(dat[,11:18]),
                  as.matrix(dat[,19:26]),deparse.level=2)

longdat <- data.frame(task=dat$Task,
                      rater= as.factor(rep(1:3,each=nrow(dat))),
                       longmat)
longdat <- longdat[, c(-1, -2)]
```



```{r question1(minmax)}
# Check whether the two data is of same class
isDataSameClass <- function(trData, teData) {
    if((class(trData) != class(teData)) & !is.null(teData)) {
        stop("Train and test data are of different class...")
    }
}

# Check whether Max greater than Min
isMaxVGreater <-  function(minV, maxV) {
    if(minV > maxV)
        stop("Maximum value argument should be larger than minimum value...")
}

# Create custome column names like V1, V2, etc
prepColNames <- function(data) {
    colname <- c()
    for(i in 1:ncol(data)) {
        colname <- c(colname, paste0("V", i))
    }
    colname
}

# Normalize train and test data in form vectors/column
getNormalized <- function(trData, teData, minV, maxV, madFlag, method) {
    
    if(!is.null(teData)) {
        if(!(is.numeric(trData)) | !is.numeric(teData)) {
            stop("Data supplied are not in numeric format...")
        }
        if(anyNA(trData) | anyNA(teData)) {
            stop("Your data contains NA...")
        }
    } else {
        if(!is.numeric(trData)) {
            stop("Data supplied are not in numeric format...")
        }
        if(anyNA(trData)) {
            stop("Your data contains NA...")
        }
    }

    trResult <- NULL
    teResult <- NULL
    result <- NULL
    
    if(method == "minmax") {
        trMin <- min(trData)
        trMax <- max(trData)
        for(i in 1:length(trData)) {
            trResult[i] <- (maxV - minV) / (trMax -  trMin) * (trData[i] - trMax) + maxV
        }
        if(!is.null(teData)) {
            for(i in 1:length(teData)) {
                teResult[[i]] <- (maxV - minV) / (trMax -  trMin) * (teData[i] - trMax) + maxV
            }
            result <- append(list(trResult), list(teResult))
        } else {
            result <- list(trResult)
        }
    } else if(method == "zscore"){
        trMean <- mean(trData)
        if(!madFlag) {
            trSd <- sd(trData)
            for(i in 1:length(trData)) {
                trResult[i] <- (trData[i] - trMean) / trSd 
            }
            if(!is.null(teData)) {
                for(i in 1:length(teData)) {
                    teResult[[i]] <- (teData[i] - trMean) / trSd 
                }
                result <- append(list(trResult), list(teResult))
            } else {
                result <- list(trResult)
            }
        } else {
            trMeanDev <- aad(trData)
            for(i in 1:length(trData)) {
                trResult[i] <- (trData[i] - trMean) / trMeanDev 
            }
            if(!is.null(teData)) {
                for(i in 1:length(teData)) {
                    teResult[[i]] <- (teData[i] - trMean) / trMeanDev 
                }
                result <- append(list(trResult), list(teResult))
            } else {
                result <- list(trResult)
            }
        }
    }
    result
}

# Normalize train and test dataframe
getNormalizedDF <- function(trData, teData, minV, maxV, madFlag, method) {
    trResultList <- list()
    teResultList <- list()
    for(i in 1:ncol(trData)) {
        if(!is.null(teData)) {
            scaled <- getNormalized(trData[, i], teData[, i], minV, maxV, madFlag, method)
            teResultList[[i]] <- scaled[2]
        } else {
            scaled <- getNormalized(trData[, i], teData, minV, maxV, madFlag, method)
        }
        trResultList[[i]] <- scaled[1]
    }
    trResult <- data.frame(trResultList)
    colnames(trResult) <- prepColNames(trResult)
    if(is.null(teData)) {
        result <- trResult
    } else {
        teResult <- data.frame(teResultList)
        colnames(teResult) <- prepColNames(teResult)
        result <- list(trResult, teResult)
    }
    result
}

# Function to compute Min Max Normalization
minmaxNorm <- function(trData = NULL, teData = NULL, minV = NULL, maxV = NULL) {
    if(!is.null(trData)) {
        isDataSameClass(trData, teData)
        isMaxVGreater(minV, maxV)
        method <- "minmax"
        if(is.vector(trData)) {
            result <- getNormalized(trData, teData, minV, maxV, NULL, method)
        } else if(is.data.frame(trData)) {
            result <- getNormalizedDF(trData, teData, minV, maxV, NULL, method)
        } else if(is.matrix(trData)) {
            if(is.null(teData)) {
                result <- getNormalizedDF(as.data.frame(trData), teData, minV, maxV, NULL, method)
            } else {
                result <- getNormalizedDF(as.data.frame(trData), 
                                                as.data.frame(teData), minV, maxV, NULL, method)
            }
        }
    } else {
        stop("Train data is required and cannot be NULL...")
    }
    result
}


# minmaxNorm(c(200, 300, 400, 600,1000), c(22, 25, 26), 3, 5)
# minmaxNorm(longdat[1:250,], NULL, 0, 1)
# minmaxNorm(longdat[1:250,], longdat[251:321,], 2, 3)
# minmaxNorm(longmat[1:250,], NULL, 0, 1)
# minmaxNorm(longmat[1:250,], longmat[251:321,], 2, 3)
```


```{r question1(zscore), warning=FALSE}
# Function to compute zscore Normalization
zscoreNorm <- function(trData = NULL, teData = NULL, madFlag = FALSE) {
    if(!is.null(trData)) {
        isDataSameClass(trData, teData)
        method <- "zscore"
        if(is.vector(trData)) {
            result <- getNormalized(trData, teData, NULL, NULL, madFlag, method)
        } else if(is.data.frame(trData)) {
            result <- getNormalizedDF(trData, teData, NULL, NULL, madFlag, method)
        } else if(is.matrix(trData)) {
            if(is.null(teData)) {
                result <- getNormalizedDF(as.data.frame(trData), teData, NULL, NULL, madFlag, method)
            } else {
                result <- getNormalizedDF(as.data.frame(trData), 
                                                as.data.frame(teData), NULL, NULL, madFlag, method)
            }
        }
    } else {
        stop("Train data is required and cannot be NULL...")
    }
    result
}

# zscoreNorm(c(200, 300, 400, 600,1000), NULL, FALSE)
# zscoreNorm(c(200, 300, 400, 600,1000), c(43, 54, 32), TRUE)
# zscoreNorm(longdat[1:250,], NULL, FALSE)
# zscoreNorm(longdat[1:250,], longdat[251:321,], FALSE)
# zscoreNorm(longmat[1:250,], NULL, FALSE)
# zscoreNorm(longmat[1:250,], longmat[251:321,], FALSE)
```


#### Question 2

(8 points) Data Mining Book: 3.6(a-c)

Report the normalized values in a table.


```{r question2}
dataFromBook <- c(200, 300, 400, 600,1000)
minmaxNorm(dataFromBook, NULL, 0, 1)
zscoreNorm(dataFromBook, NULL, FALSE)
zscoreNorm(dataFromBook, NULL, TRUE)
```



#### Question 3

Load the Iris data set available from the UCI Machine Learning data repository, http://archive.ics.uci.edu/ml/.

Using the data for petal length, answer the following questions:

#### Question 3 a

(4 points) Use your min-max normalization function with a range [-1.0; 1.0], to what values would {1.95, 3.1, 5.68 and 6.2} transform?


```{r question3a}
# Reading the data 
destfile <- "data/iris.csv"

# checking whether the file already exists or not
if(!file.exists(destfile)) {
    iris <- read.csv(url("http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"), col.names = c("sepal.length", "sepal.width", "petal.length", "petal.width", "class"), header = F)
    write.csv(iris, file = "data/iris.csv")
} else {
    iris <- read.csv("data/iris.csv", header = T)
}

dataQ3a <- c(1.95, 3.1, 5.68, 6.2)
result <- minmaxNorm(iris$petal.length, dataQ3a, -1, 1)
result
```


#### Question 3 b

(4 points) Use your z-score normalization function to determine what values {1.95, 3.1, 5.68, and 6.2} would transform to?


```{r question3b}
result <- zscoreNorm(iris$petal.length, dataQ3a, TRUE)
result
```


#### Question 3 c

(2 points) Comment on which method is preferred for this data, and why?


**Reasons**


#### Question 4

Consider the following data set of with 5 samples and 3 variables:

```{r}
A <- c(1.4,1.8,1.3,0.9,1.5)
B <- c(1.3,1.4,1.2,3.5,2.1)
C <- c(2.9,3.2,2.9,3.1,3.3)

dat <- cbind.data.frame(A,B,C)

rownames(dat) <- c("x1","x2","x3","x4","x5")
dat
```

You have a new data point x = (1:25; 1:78; 3:01).
```{r}
x <- rbind.data.frame(c(1.25,1.78,3.01))
```


#### Question 4 a

(5 points) Calculate and present the distance between the new data point and each of the points in the data set using Manhattan distance, Euclidean distance, Minkowski distance ( = 3), supremum distance, and cosine similarity.


```{r question4a}
g <- rep(0,5)
```
```{r Manhattan Distance}
# Function to compute Manhattan Distance
manhattanDist <- function(data1,data2){
        for(i in 1:nrow(data1)){
                p <- rep(0,3)
                for(j in 1:ncol(data2)){
                        
                        p[j] <- abs(data1[i,j] - data2[1,j])
                        
                }
                g[i] <- sum(p)
        }
        return(g)
}

result <- manhattanDist(dat,x)
manhattanRes <- data.frame(matrix(data = result,nrow = 5,ncol = 1,byrow = T),row.names = c("x1","x2","x3","x4","x5"))
colnames(manhattanRes) <- c("x(Manhattan Distance)")

manhattanRes
```


```{r Euclidean Distance}
# Function to compute Euclidean Distance
euclideanDist <- function(data1,data2){
        for(i in 1:nrow(data1)){
                p <- rep(0,3)
                for(j in 1:ncol(data2)){
                       
                        p[j] <- (data1[i,j] - data2[1,j])^2
                        
                }
                g[i] <- sqrt(sum(p))
        }
        return(g)
}

result <- euclideanDist(dat,x)
euclideanRes <- data.frame(matrix(data = result,nrow = 5,ncol = 1,byrow = T),row.names = c("x1","x2","x3","x4","x5"))
colnames(euclideanRes) <- c("x(Euclidean Distance)")

euclideanRes
```


```{r Minkowski Distance}
# Function to compute Minkowski distance
minkowskiDist <- function(data1,data2,lambda){
        for(i in 1:nrow(data1)){
                p <- rep(0,3)
                for(j in 1:ncol(data2)){
                        
                        p[j] <- (abs(data1[i,j] - data2[1,j]))^lambda
                        
                }
                g[i] <- (sum(p))^(1/lambda)
        }
        return(g)
}
result <- minkowskiDist(dat,x,lambda = 3)
minkowskiRes <- data.frame(matrix(data = result,nrow = 5,ncol = 1,byrow = T),row.names = c("x1","x2","x3","x4","x5"))
colnames(minkowskiRes) <- c("x(Minkowski Distance)")

minkowskiRes
```


```{r Supremum Distance}
# Function to compute supremum distance
supremumDist <- function(data1,data2){
        for(i in 1:nrow(data1)){
                p <- rep(0,3)
                for(j in 1:ncol(data2)){
                        
                        p[j] <- abs(data1[i,j] - data2[1,j])
                        
                }
                g[i] <- max(p)
        }
        return(g)
}

result <- supremumDist(dat,x)
supremumRes <- data.frame(matrix(data = result,nrow = 5,ncol = 1,byrow = T),row.names = c("x1","x2","x3","x4","x5"))
colnames(supremumRes) <- c("x(Supremum Distance)")

supremumRes
```


```{r Cosine Similarity}
#Cosine Similarity
cosineSimilarity <- function(data1,data2){
        for(i in 1:nrow(data1)){
                p <- rep(0,3)
                q <- rep(0,3)
                r <- rep(0,3)
                res <- rep(0,0)
                #r <- rep(0,3)
                for(j in 1:ncol(data2)){
                       
                        p[j] <- sum(data1[i,j] * data2[1,j])
                        q[j] <- sqrt(sum(data1[i,j])^2)
                        r[j] <- sqrt(sum(data2[1,j])^2)
                        res[j] <- p[j]/(q[j] * r[j])
                }
                g[i] <- res
        }
        return(g)
}

cosineSimilarity(dat,x)
```


#### Question 4 b

(5 points) Normalize the data using min-max normalization to be between 0 and 1. What is the Euclidean distance between the new data point and x1; : : : ; x5.


```{r question4b}
#min-max normalization for the data
dat1 <- data.frame(minmaxNorm(dat,NULL,0,1))
dat1
```

```{r}
# Euclidean Distance
result <- euclideanDist(dat1,x)
euclideanRes <- data.frame(matrix(data = result,nrow = 5,ncol = 1,byrow = T),row.names = c("x1","x2","x3","x4","x5"))
colnames(euclideanRes) <- c("x")

euclideanRes
```


#### Question 5

**K-means Clustering**

Perform k-means clustering manually with k=2 on the example data given below of n = 8 samples over p = 2 features.

Table here


#### Question 5 a

(2 points) Plot the sample data.


```{r question5a}

```


#### Question 5 b

(4 points) Assign samples to be the initial groupings given in the table. Compute and report the centroid for each cluster.

```{r question5b}

```


#### Question 5 c

(4 points) Assign each sample to the centroid to which it is closest (Euclidean distance). Report the cluster labels for each observation.

```{r question5c}

```


#### Question 5 d

(20 points) Repeat (b) and (c) until the clusters remain stable.


```{r question5d}

```


#### Question 5 e

(2 points) Plot the sample data colored by cluster labeling and adding centroid points.


```{r question5e}

```


#### Question 6

**Hierarchical Clustering**

Suppose you have 5 samples, for which the dissimilarity matrix is shown below:

Matrix here

That is, the distance between the first and second sample is 0.3; the distance between the first and fourth sample is 0.7.


#### Question 6 a

(12 points) Trace running through hierarchical clustering manually with complete link- age and sketch the dendrogram. Estimate the heights in the dendrogram from the dissimilarity distances.


```{r question6a}

```


#### Question 6 b

(12 points) Repeat (a), with single linkage clustering


```{r question6b}

```


#### Question 6 c

(6 points) Use the dendrogram from (a) and (b), cut the dendrograms to form three clusters. Which samples are in each cluster?


```{r question6c}

```


**End of assignemnt**